Project Path: arc_archercreat_titan_5iexas0g

Source Tree:

```txt
arc_archercreat_titan_5iexas0g
├── CMakeLists.txt
├── README.md
├── intrinsics
│   ├── CMakeLists.txt
│   ├── flags.hpp
│   ├── main.cpp
│   └── remill
├── shell.nix
└── src
    ├── asserts.hpp
    ├── binary.cpp
    ├── binary.hpp
    ├── emulator.cpp
    ├── emulator.hpp
    ├── explorer.cpp
    ├── explorer.hpp
    ├── il
    │   ├── optimizer.cpp
    │   ├── optimizer.hpp
    │   ├── passes
    │   │   ├── alias.cpp
    │   │   ├── alias.hpp
    │   │   ├── coalescing.cpp
    │   │   ├── coalescing.hpp
    │   │   ├── deps.cpp
    │   │   ├── deps.hpp
    │   │   ├── flags_synthesis.cpp
    │   │   └── flags_synthesis.hpp
    │   ├── solver.cpp
    │   └── solver.hpp
    ├── lifter.cpp
    ├── lifter.hpp
    ├── logger.hpp
    ├── main.cpp
    ├── tracer.cpp
    ├── tracer.hpp
    ├── utils.cpp
    ├── utils.hpp
    └── vm
        ├── instruction.cpp
        ├── instruction.hpp
        ├── routine.cpp
        └── routine.hpp

```

`CMakeLists.txt`:

```txt
cmake_minimum_required(VERSION 3.15)

project(titan)

# Packages
find_package(fmt CONFIG REQUIRED)
find_package(LLVM 15.0 CONFIG REQUIRED)
find_package(triton CONFIG REQUIRED)
find_package(range-v3 CONFIG REQUIRED)

file(GLOB_RECURSE TITAN_SOURCES  CONFIGURE_DEPENDS "${CMAKE_CURRENT_SOURCE_DIR}/src/*.cpp")
file(GLOB_RECURSE TITAN_INCLUDES CONFIGURE_DEPENDS "${CMAKE_CURRENT_SOURCE_DIR}/src/*.hpp")
source_group(TREE ${PROJECT_SOURCE_DIR} FILES ${TITAN_SOURCES} ${TITAN_INCLUDES})

add_executable(${PROJECT_NAME} ${TITAN_SOURCES} ${TITAN_INCLUDES})

target_include_directories(${PROJECT_NAME} PRIVATE
  "src/"
)
target_compile_features(${PROJECT_NAME} PRIVATE 
  cxx_std_20
)
target_link_libraries(${PROJECT_NAME} PRIVATE 
  fmt::fmt
  range-v3 
  triton::triton
)

add_subdirectory(intrinsics)

```

`README.md`:

```md
# titan - VMProtect devirtualizer

I'm releasing my VMProtect devirtualizer for others to research, learn, and improve. This project started in 2018 as a hobby project and was rewritten at least 4 times. During my research, I've met with awesome 
people, made friends, and learned a lot. The tool is for educational purposes only, it works for vmprotect < 3.8 but produces less than ideal output.

## How does it work?

The tool uses [Triton](https://github.com/JonathanSalwan/Triton) for emulation, symbolic execution, and lifting. The easiest way to match VM handlers is to match them on the Triton AST level. The tool symbolizes vip and vsp registers and propagates memory loads and stores. Almost every handler ends with the store (to the stack, vm register or memory). We take Triton AST of the value that is being stored and match against known patterns:
```c
// Match [vsp] + [vsp].
//
static bool match_add(const triton::ast::SharedAbstractNode& ast)
{
    if (ast->getType() == triton::ast::EXTRACT_NODE)
    {
        return match_add(ast->getChildren()[2]->getChildren()[1]);
    }
    return ast->getType() == triton::ast::BVADD_NODE
        && is_variable(ast->getChildren()[1], variable::vsp_fetch);
}
```

No matter how obfuscated handlers are, it is possible to match them with a single x86 instruction! Once the handler is identified, it is lifted into a basic block. Once the basic block is terminated, the partial control-flow graph is computed and the RIP register is sliced, giving the address of the next basic block. The process repeats until no new basic blocks are found.
Every basic block is lifted into separate LLVM function. The process of building control-flow graph comes down chaining calls to basic block functions in the right order.
The tool has few custom LLVM passes like `no-alias` and `memory coalescing` passes. The only pass that is left to implement is `flag synthesis` pass which will give the cleanest LLVM bitcode.

## Usage

The tool requires 3 arguments:
- Path to vmprotect intrinsics file
- Path to virtualized binary
- Virtual address of vm entry point
```
./build/titan
titan: for the -i option: must be specified at least once!
titan: for the -b option: must be specified at least once!
titan: for the -e option: must be specified at least once!
./build/titan -i intrinsics/vmprotect64.ll -b samples/loop_hash.0x140103FF4.exe -e 0x140103FF4
```

## Acknowledgements

_Matteo Favaro_ and _Vlad Malagar_ for answering my sometimes dumb questions, helping to find bugs in llvm bitcode, giving motivation and new ideas.

```

`intrinsics/CMakeLists.txt`:

```txt
cmake_minimum_required(VERSION 3.21)
project(intrinsics)

SET (CMAKE_CXX_COMPILER "clang++")

set(HELPER_CLANG_FLAGS
    -std=c++17
    -O3
    -Iremill/include
    -Wno-gnu-inline-cpp-without-extern
    -fno-discard-value-names
    -fstrict-aliasing
    -fno-slp-vectorize
    -mllvm
    -enable-tbaa=true
    -emit-llvm
)

set(SOURCES "main.cpp")

add_custom_command(OUTPUT
    "${CMAKE_CURRENT_BINARY_DIR}/vmprotect64.ll"
    "${CMAKE_CURRENT_BINARY_DIR}/vmprotect32.ll"
    COMMAND "${CMAKE_CXX_COMPILER}" ${HELPER_CLANG_FLAGS} -DADDRESS_SIZE_BITS=64 -m64 -S main.cpp -o vmprotect32.ll
    COMMAND "${CMAKE_CXX_COMPILER}" ${HELPER_CLANG_FLAGS} -DADDRESS_SIZE_BITS=64 -m64 -S main.cpp -o vmprotect64.ll
    MAIN_DEPENDENCY ${SOURCES}
    DEPENDS ${SOURCES}
    WORKING_DIRECTORY "${CMAKE_CURRENT_SOURCE_DIR}"
)

add_custom_target(${PROJECT_NAME} ALL
    DEPENDS vmprotect64.ll vmprotect32.ll
    SOURCES ${SOURCES}
)

```

`intrinsics/flags.hpp`:

```hpp
#pragma once

// Auxiliary carry flag. This is used for binary coded decimal operations and
// is the 5th bit (where each binary decimal is 4 bits).
//
template <typename T>
inline static bool AuxCarryFlag(T lhs, T rhs, T res)
{
    return ((res ^ lhs ^ rhs) & T(0x10));
}

// Auxiliary carry flag. This is used for binary coded decimal operations and
// is the 5th bit (where each binary decimal is 4 bits).
//
template <typename T>
inline static bool AuxCarryFlag(T lhs, T rhs, T carry, T res)
{
    return ((res ^ lhs ^ carry ^ rhs) & T(0x10));
}

// Tests whether there is an even number of bits in the low order byte.
//
inline static bool ParityFlag(uint8_t r0)
{
    return !__builtin_parity(static_cast<unsigned>(r0));
}

// Tests whether there is an even number of bits or not.
//
template <typename T>
inline static bool ParityFlag(T x)
{
    return ParityFlag(static_cast<uint8_t>(x));
}

// Sign flag, tells us if a result is signed or unsigned.
//
template <typename T>
inline static bool SignFlag(T res)
{
    return 0 > Signed(res);
}

// Zero flags, tells us whether or not a value is zero.
//
template <typename T>
inline static bool ZeroFlag(T res)
{
    return T(0) == res;
}

// Calculate the carry flag for SHLD.
//
template <typename T>
inline static uint8_t SHLDCarryFlag(T val, T count)
{
    return UCmpEq(UAnd(UShr(val, USub(BitSizeOf(count), count)), 1), 1);
}

// Calculate the carry flag for SHRD.
//
template <typename T>
inline static uint8_t SHRDCarryFlag(T val, T count)
{
    return UCmpEq(UAnd(UShr(val, USub(count, 1)), 1), 1);
}

struct tag_add {};
struct tag_div {};
struct tag_mul {};

// Generic carry flag.
//
template <typename Tag>
struct Carry;

// Computes an carry flag when two numbers are added together.
//
template <>
struct Carry<tag_add>
{
    template <typename T>
    inline static bool Flag(T lhs, T rhs, T res)
    {
        static_assert(std::is_unsigned<T>::value, "Invalid specialization of `Carry::Flag` for addition.");
        return res < lhs || res < rhs;
    }
};

// Generic overflow flag.
//
template <typename T>
struct Overflow;

// Computes an overflow flag when two numbers are added together.
//
template <>
struct Overflow<tag_add>
{
    template <typename T>
    inline static bool Flag(T lhs, T rhs, T res)
    {
        static_assert(std::is_unsigned<T>::value, "Invalid specialization of `Overflow::Flag` for addition.");
        enum
        {
            kSignShift = sizeof(T) * 8 - 1
        };
        const T sign_lhs = lhs >> kSignShift;
        const T sign_rhs = rhs >> kSignShift;
        const T sign_res = res >> kSignShift;
        return 2 == (sign_lhs ^ sign_res) + (sign_rhs ^ sign_res);
    }
};

// Computes an overflow flag when one number is multiplied with another.
//
template <>
struct Overflow<tag_mul>
{
    // Integer multiplication overflow check, where result is twice the width of the operands.
    //
    template <typename T, typename R>
    inline static bool Flag(T, T, R res, typename std::enable_if<sizeof(T) < sizeof(R),int>::type=0)
    {
        return static_cast<R>(static_cast<T>(res)) != res;
    }

    // Signed integer multiplication overflow check, where the result is
    // truncated to the size of the operands.
    //
    template <typename T>
    inline static bool Flag(T lhs, T rhs, T, typename std::enable_if<std::is_signed<T>::value,int>::type=0)
    {
        auto lhs_wide = SExt(lhs);
        auto rhs_wide = SExt(rhs);
        return Flag<T, decltype(lhs_wide)>(lhs, rhs, lhs_wide * rhs_wide);
    }
};

```

`intrinsics/main.cpp`:

```cpp
#include <stdint.h>
#include <stdio.h>
#include <type_traits>
#include <cstring>

#include <cpuid.h>
#include <x86intrin.h>

#include <remill/Arch/Runtime/Intrinsics.h>
#include <remill/Arch/Runtime/Operators.h>
#include <remill/Arch/Runtime/Types.h>

#include "flags.hpp"

// Useful intrinsics @ https://clang.llvm.org/docs/LanguageExtensions.html
// The semantics are exported to the lifter with the SEM_ prefix.
//
#define DEFINE_SEMANTIC(name) extern "C" constexpr auto SEM_##name [[gnu::used]]
#if ADDRESS_SIZE_BITS == 64
#define DEFINE_SEMANTIC_32(name) DEFINE_SEMANTIC(UNDEF_##name)
#define DEFINE_SEMANTIC_64(name) DEFINE_SEMANTIC(name)
#else
#define DEFINE_SEMANTIC_32(name) DEFINE_SEMANTIC(name)
#define DEFINE_SEMANTIC_64(name) DEFINE_SEMANTIC(UNDEF_##name)
#endif
#define TYPE_BITSIZE(type) sizeof(type) * 8;

#define INLINE __attribute__((always_inline))
#define CONST  __attribute__((const))

// Structure to represent a VMProtect virtual register.
//
IF_64BIT(
struct VirtualRegister final
{
    union
    {
        alignas(1) struct
        {
            uint8_t b0;
            uint8_t b1;
            uint8_t b2;
            uint8_t b3;
            uint8_t b4;
            uint8_t b5;
            uint8_t b6;
            uint8_t b7;
        } byte;
        alignas(2) struct
        {
            uint16_t w0;
            uint16_t w1;
            uint16_t w2;
            uint16_t w3;
        } word;
        alignas(4) struct
        {
            uint32_t d0;
            uint32_t d1;
        } dword;
        alignas(8) uint64_t qword;
    } __attribute__((packed));
} __attribute__((packed));
)

IF_32BIT(
struct VirtualRegister final
{
    union
    {
        alignas(1) struct
        {
            uint8_t b0;
            uint8_t b1;
            uint8_t b2;
            uint8_t b3;
        } byte;
        alignas(2) struct
        {
            uint16_t w0;
            uint16_t w1;
        } word;
        alignas(4) uint32_t dword;
    } __attribute__((packed));
} __attribute__((packed));
)

static_assert(sizeof(VirtualRegister) * 8 == ADDRESS_SIZE_BITS, "VirtualRegister size has to be equal to address size");

extern "C" uint8_t RAM[0];
extern "C" uint8_t GS[0];
extern "C" uint8_t FS[0];

// Define NoAlias pointers.
//
using rptr = size_t &__restrict__;

// Structure to represent a virtual context.
//
IF_64BIT(
struct VirtualContext final
{
    VirtualRegister rax;
    VirtualRegister rbx;
    VirtualRegister rcx;
    VirtualRegister rdx;
    VirtualRegister rsi;
    VirtualRegister rdi;
    VirtualRegister rbp;
    VirtualRegister rsp;
    VirtualRegister r8;
    VirtualRegister r9;
    VirtualRegister r10;
    VirtualRegister r11;
    VirtualRegister r12;
    VirtualRegister r13;
    VirtualRegister r14;
    VirtualRegister r15;
} __attribute__((packed));
)

IF_32BIT(
struct VirtualContext final
{
    VirtualRegister eax;
    VirtualRegister ebx;
    VirtualRegister ecx;
    VirtualRegister edx;
    VirtualRegister esi;
    VirtualRegister edi;
    VirtualRegister ebp;
    VirtualRegister esp;
} __attribute__((packed));
)

// Undefine function, it must return 'undef' at runtime.
//
extern "C" size_t __undef;

template <typename T>
INLINE T UNDEF()
{
    return (T)__undef;
}

// Stack push/pop semantic.
//
template <typename T>
INLINE void STACK_PUSH(size_t &vsp, T value)
{
    // 1. Update the stack pointer.
    //
    vsp -= sizeof(T);
    // 2. Store the value.
    //
    std::memcpy(&RAM[vsp], &value, sizeof(T));
}

template <typename T>
INLINE T STACK_POP(size_t &vsp)
{
    // 1. Fetch the value.
    //
    T value = 0;
    std::memcpy(&value, &RAM[vsp], sizeof(T));
    // 2. Undefine the stack slot.
    //
    T undef = UNDEF<T>();
    std::memcpy(&RAM[vsp], &undef, sizeof(T));
    // 3. Update the stack pointer.
    //
    vsp += sizeof(T);
    // 4. Return the value.
    //
    return value;
}

DEFINE_SEMANTIC_64(STACK_POP_64) = STACK_POP<uint64_t>;
DEFINE_SEMANTIC_32(STACK_POP_32) = STACK_POP<uint32_t>;

// Immediate and symbolic push/pop semantic.
//
template <typename T>
INLINE void PUSH_IMM(size_t &vsp, T value)
{
    STACK_PUSH<T>(vsp, value);
}

DEFINE_SEMANTIC_64(PUSH_IMM_64) = PUSH_IMM<uint64_t>;
DEFINE_SEMANTIC(PUSH_IMM_32)    = PUSH_IMM<uint32_t>;
DEFINE_SEMANTIC(PUSH_IMM_16)    = PUSH_IMM<uint16_t>;

// Stack pointer push/pop semantic.
//
IF_64BIT(
template <size_t size>
INLINE void PUSH_VSP(size_t &vsp)
{
    // 1. Push the stack pointer.
    //
    if constexpr (size == 64)
    {
        STACK_PUSH<uint64_t>(vsp, vsp);
    }
    else if constexpr (size == 32)
    {
        STACK_PUSH<uint32_t>(vsp, vsp & 0xFFFFFFFF);
    }
    else if constexpr (size == 16)
    {
        STACK_PUSH<uint16_t>(vsp, vsp & 0xFFFF);
    }
})

IF_32BIT(
template <size_t size>
INLINE void PUSH_VSP(size_t &vsp)
{
    // 1. Push the stack pointer.
    //
    if constexpr (size == 32)
    {
        STACK_PUSH<uint32_t>(vsp, vsp);
    }
    else if constexpr (size == 16)
    {
        STACK_PUSH<uint16_t>(vsp, vsp & 0xFFFF);
    }
}
)

DEFINE_SEMANTIC_64(PUSH_VSP_64) = PUSH_VSP<64>;
DEFINE_SEMANTIC(PUSH_VSP_32)    = PUSH_VSP<32>;
DEFINE_SEMANTIC(PUSH_VSP_16)    = PUSH_VSP<16>;

IF_64BIT(
template <size_t size>
INLINE void POP_VSP(size_t &vsp)
{
    // 1. Push the stack pointer.
    //
    if constexpr (size == 64)
    {
        vsp = STACK_POP<uint64_t>(vsp);
    }
    else if constexpr (size == 32)
    {
        uint32_t value = STACK_POP<uint32_t>(vsp);
        vsp = ((vsp & 0xFFFFFFFF00000000) | value);
    }
    else if constexpr (size == 16)
    {
        uint16_t value = STACK_POP<uint16_t>(vsp);
        vsp = ((vsp & 0xFFFFFFFFFFFF0000) | value);
    }
}
)

IF_32BIT(
template <size_t size>
INLINE void POP_VSP(size_t &vsp)
{
    // 1. Push the stack pointer.
    //
    if constexpr (size == 32)
    {
        vsp = STACK_POP<uint32_t>(vsp);
    }
    else if constexpr (size == 16)
    {
        uint16_t value = STACK_POP<uint16_t>(vsp);
        vsp = ((vsp & 0xFFFF0000) | value);
    }
}
)

DEFINE_SEMANTIC_64(POP_VSP_64) = POP_VSP<64>;
DEFINE_SEMANTIC(POP_VSP_32)    = POP_VSP<32>;
DEFINE_SEMANTIC(POP_VSP_16)    = POP_VSP<16>;

// Flags pop semantic.
//
INLINE void POP_FLAGS(size_t &vsp, size_t &eflags)
{
    // 1. Pop the eflags.
    //
    eflags = STACK_POP<size_t>(vsp);
}

DEFINE_SEMANTIC(POP_FLAGS) = POP_FLAGS;

// Stack load/store semantic.
//
template <typename T>
INLINE void LOAD(size_t &vsp)
{
    // 1. Check if it's 'byte' size.
    //
    bool isByte = (sizeof(T) == 1);
    // 2. Pop the address.
    //
    size_t address = STACK_POP<size_t>(vsp);
    // 3. Load the value.
    //
    T value = 0;
    std::memcpy(&value, &RAM[address], sizeof(T));
    // 4. Save the result.
    //
    if (isByte)
    {
        STACK_PUSH<uint16_t>(vsp, ZExt(value));
    }
    else
    {
        STACK_PUSH<T>(vsp, value);
    }
}

DEFINE_SEMANTIC_64(LOAD_SS_64) = LOAD<uint64_t>;
DEFINE_SEMANTIC(LOAD_SS_32)    = LOAD<uint32_t>;
DEFINE_SEMANTIC(LOAD_SS_16)    = LOAD<uint16_t>;
DEFINE_SEMANTIC(LOAD_SS_8)     = LOAD<uint8_t>;

DEFINE_SEMANTIC_64(LOAD_DS_64) = LOAD<uint64_t>;
DEFINE_SEMANTIC(LOAD_DS_32)    = LOAD<uint32_t>;
DEFINE_SEMANTIC(LOAD_DS_16)    = LOAD<uint16_t>;
DEFINE_SEMANTIC(LOAD_DS_8)     = LOAD<uint8_t>;

DEFINE_SEMANTIC_64(LOAD_64) = LOAD<uint64_t>;
DEFINE_SEMANTIC(LOAD_32)    = LOAD<uint32_t>;
DEFINE_SEMANTIC(LOAD_16)    = LOAD<uint16_t>;
DEFINE_SEMANTIC(LOAD_8)     = LOAD<uint8_t>;

template <typename T>
INLINE void LOAD_GS(size_t &vsp)
{
    // 1. Check if it's 'byte' size.
    //
    bool isByte = (sizeof(T) == 1);
    // 2. Pop the address.
    //
    size_t address = STACK_POP<size_t>(vsp);
    // 3. Load the value.
    //
    T value = 0;
    std::memcpy(&value, &GS[address], sizeof(T));
    // 4. Save the result.
    //
    if (isByte)
    {
        STACK_PUSH<uint16_t>(vsp, ZExt(value));
    }
    else
    {
        STACK_PUSH<T>(vsp, value);
    }
}

DEFINE_SEMANTIC_64(LOAD_GS_64) = LOAD_GS<uint64_t>;
DEFINE_SEMANTIC(LOAD_GS_32)    = LOAD_GS<uint32_t>;
DEFINE_SEMANTIC(LOAD_GS_16)    = LOAD_GS<uint16_t>;
DEFINE_SEMANTIC(LOAD_GS_8)     = LOAD_GS<uint8_t>;

template <typename T>
INLINE void LOAD_FS(size_t &vsp)
{
    // 1. Check if it's 'byte' size.
    //
    bool isByte = (sizeof(T) == 1);
    // 2. Pop the address.
    //
    size_t address = STACK_POP<size_t>(vsp);
    // 3. Load the value.
    //
    T value = 0;
    std::memcpy(&value, &FS[address], sizeof(T));
    // 4. Save the result.
    //
    if (isByte)
    {
        STACK_PUSH<uint16_t>(vsp, ZExt(value));
    }
    else
    {
        STACK_PUSH<T>(vsp, value);
    }
}

DEFINE_SEMANTIC_64(LOAD_FS_64) = LOAD_FS<uint64_t>;
DEFINE_SEMANTIC(LOAD_FS_32)    = LOAD_FS<uint32_t>;
DEFINE_SEMANTIC(LOAD_FS_16)    = LOAD_FS<uint16_t>;
DEFINE_SEMANTIC(LOAD_FS_8)     = LOAD_FS<uint8_t>;

template <typename T>
INLINE void STORE(size_t &vsp)
{
    // 1. Check if it's 'byte' size.
    //
    bool isByte = (sizeof(T) == 1);
    // 2. Pop the address.
    //
    size_t address = STACK_POP<size_t>(vsp);
    // 3. Pop the value.
    //
    T value;
    if (isByte)
    {
        value = Trunc(STACK_POP<uint16_t>(vsp));
    }
    else
    {
        value = STACK_POP<T>(vsp);
    }
    // 4. Store the value.
    //
    std::memcpy(&RAM[address], &value, sizeof(T));
}

DEFINE_SEMANTIC_64(STORE_SS_64) = STORE<uint64_t>;
DEFINE_SEMANTIC(STORE_SS_32)    = STORE<uint32_t>;
DEFINE_SEMANTIC(STORE_SS_16)    = STORE<uint16_t>;
DEFINE_SEMANTIC(STORE_SS_8)     = STORE<uint8_t>;

DEFINE_SEMANTIC_64(STORE_DS_64) = STORE<uint64_t>;
DEFINE_SEMANTIC(STORE_DS_32)    = STORE<uint32_t>;
DEFINE_SEMANTIC(STORE_DS_16)    = STORE<uint16_t>;
DEFINE_SEMANTIC(STORE_DS_8)     = STORE<uint8_t>;

DEFINE_SEMANTIC_64(STORE_64) = STORE<uint64_t>;
DEFINE_SEMANTIC(STORE_32)    = STORE<uint32_t>;
DEFINE_SEMANTIC(STORE_16)    = STORE<uint16_t>;
DEFINE_SEMANTIC(STORE_8)     = STORE<uint8_t>;

// Virtual register push/pop semantic.
//
IF_64BIT(
template <size_t size, size_t offset>
INLINE void PUSH_VREG(size_t &vsp, VirtualRegister vmreg)
{
    // 1. Update the stack pointer.
    //
    vsp -= ((size != 8) ? (size / 8) : ((size / 8) * 2));
    // 2. Select the proper element of the virtual register.
    //
    if constexpr (size == 64)
    {
        std::memcpy(&RAM[vsp], &vmreg.qword, sizeof(uint64_t));
    }
    else if constexpr (size == 32)
    {
        if constexpr (offset == 0)
        {
            std::memcpy(&RAM[vsp], &vmreg.dword.d0, sizeof(uint32_t));
        }
        else if constexpr (offset == 1)
        {
            std::memcpy(&RAM[vsp], &vmreg.dword.d1, sizeof(uint32_t));
        }
    }
    else if constexpr (size == 16)
    {
        if constexpr (offset == 0)
        {
            std::memcpy(&RAM[vsp], &vmreg.word.w0, sizeof(uint16_t));
        }
        else if constexpr (offset == 1)
        {
            std::memcpy(&RAM[vsp], &vmreg.word.w1, sizeof(uint16_t));
        }
        else if constexpr (offset == 2)
        {
            std::memcpy(&RAM[vsp], &vmreg.word.w2, sizeof(uint16_t));
        }
        else if constexpr (offset == 3)
        {
            std::memcpy(&RAM[vsp], &vmreg.word.w3, sizeof(uint16_t));
        }
    }
    else if constexpr (size == 8)
    {
        if constexpr (offset == 0)
        {
            uint16_t byte = ZExt(vmreg.byte.b0);
            std::memcpy(&RAM[vsp], &byte, sizeof(uint16_t));
        }
        else if constexpr (offset == 1)
        {
            uint16_t byte = ZExt(vmreg.byte.b1);
            std::memcpy(&RAM[vsp], &byte, sizeof(uint16_t));
        }
        // NOTE: there might be other offsets here, but they were not observed.
        //
    }
}
)

IF_32BIT(
template <size_t size, size_t offset>
INLINE void PUSH_VREG(size_t &vsp, VirtualRegister vmreg)
{
    // 1. Update the stack pointer.
    //
    vsp -= ((size != 8) ? (size / 8) : ((size / 8) * 2));
    // 2. Select the proper element of the virtual register.
    //
    if constexpr (size == 32)
    {
        if constexpr (offset == 0)
        {
            std::memcpy(&RAM[vsp], &vmreg.dword, sizeof(uint32_t));
        }
    }
    else if constexpr (size == 16)
    {
        if constexpr (offset == 0)
        {
            std::memcpy(&RAM[vsp], &vmreg.word.w0, sizeof(uint16_t));
        }
        else if constexpr (offset == 1)
        {
            std::memcpy(&RAM[vsp], &vmreg.word.w1, sizeof(uint16_t));
        }
    }
    else if constexpr (size == 8)
    {
        if constexpr (offset == 0)
        {
            uint16_t byte = ZExt(vmreg.byte.b0);
            std::memcpy(&RAM[vsp], &byte, sizeof(uint16_t));
        }
        else if constexpr (offset == 1)
        {
            uint16_t byte = ZExt(vmreg.byte.b1);
            std::memcpy(&RAM[vsp], &byte, sizeof(uint16_t));
        }
        // NOTE: there might be other offsets here, but they were not observed.
        //
    }
}
)

DEFINE_SEMANTIC(PUSH_VREG_8_0)     = PUSH_VREG<8, 0>;
DEFINE_SEMANTIC(PUSH_VREG_8_1)     = PUSH_VREG<8, 1>;
DEFINE_SEMANTIC(PUSH_VREG_16_0)    = PUSH_VREG<16, 0>;
DEFINE_SEMANTIC(PUSH_VREG_16_2)    = PUSH_VREG<16, 1>;

// DEFINE_SEMANTIC_64(PUSH_VREG_8_1)  = PUSH_VREG<8,  1>;
DEFINE_SEMANTIC_64(PUSH_VREG_16_4) = PUSH_VREG<16, 2>;
DEFINE_SEMANTIC_64(PUSH_VREG_16_6) = PUSH_VREG<16, 3>;
DEFINE_SEMANTIC_64(PUSH_VREG_32_0) = PUSH_VREG<32, 0>;
DEFINE_SEMANTIC_32(PUSH_VREG_32)   = PUSH_VREG<32, 0>;
DEFINE_SEMANTIC_64(PUSH_VREG_32_4) = PUSH_VREG<32, 1>;
DEFINE_SEMANTIC_64(PUSH_VREG_64_0) = PUSH_VREG<64, 0>;

IF_64BIT(
template <size_t size, size_t offset>
INLINE void POP_VREG(size_t &vsp, VirtualRegister &vmreg)
{
    // 1. Fetch and store the value on the virtual register.
    //
    if constexpr (size == 64)
    {
        uint64_t value = 0;
        std::memcpy(&value, &RAM[vsp], sizeof(uint64_t));
        vmreg.qword = value;
    } else if constexpr (size == 32)
    {
        if constexpr (offset == 0)
        {
            uint32_t value = 0;
            std::memcpy(&value, &RAM[vsp], sizeof(uint32_t));
            vmreg.qword = ((vmreg.qword & 0xFFFFFFFF00000000) | value);
        }
        else if constexpr (offset == 1)
        {
            uint32_t value = 0;
            std::memcpy(&value, &RAM[vsp], sizeof(uint32_t));
            vmreg.qword = ((vmreg.qword & 0x00000000FFFFFFFF) | UShl(ZExt(value), 32));
        }
    }
    else if constexpr (size == 16)
    {
        if constexpr (offset == 0)
        {
            uint16_t value = 0;
            std::memcpy(&value, &RAM[vsp], sizeof(uint16_t));
            vmreg.qword = ((vmreg.qword & 0xFFFFFFFFFFFF0000) | value);
        }
        else if constexpr (offset == 1)
        {
            uint16_t value = 0;
            std::memcpy(&value, &RAM[vsp], sizeof(uint16_t));
            vmreg.qword = ((vmreg.qword & 0xFFFFFFFF0000FFFF) | UShl(ZExtTo<uint64_t>(value), 16));
        }
        else if constexpr (offset == 2)
        {
            uint16_t value = 0;
            std::memcpy(&value, &RAM[vsp], sizeof(uint16_t));
            vmreg.qword = ((vmreg.qword & 0xFFFF0000FFFFFFFF) | UShl(ZExtTo<uint64_t>(value), 32));
        }
        else if constexpr (offset == 3)
        {
            uint16_t value = 0;
            std::memcpy(&value, &RAM[vsp], sizeof(uint16_t));
            vmreg.qword = ((vmreg.qword & 0x0000FFFFFFFFFFFF) | UShl(ZExtTo<uint64_t>(value), 48));
        }
    }
    else if constexpr (size == 8)
    {
        if constexpr (offset == 0)
        {
            uint16_t byte = 0;
            std::memcpy(&byte, &RAM[vsp], sizeof(uint16_t));
            vmreg.byte.b0 = Trunc(byte);
        }
        else if constexpr (offset == 1)
        {
            uint16_t byte = 0;
            std::memcpy(&byte, &RAM[vsp], sizeof(uint16_t));
            vmreg.byte.b1 = Trunc(byte);
        }
        // NOTE: there might be other offsets here, but they were not observed.
        //
    }
    // 4. Clear the value on the stack.
    //
    if constexpr (size == 64)
    {
        uint64_t undef = UNDEF<uint64_t>();
        std::memcpy(&RAM[vsp], &undef, sizeof(uint64_t));
    }
    else if constexpr (size == 32)
    {
        uint32_t undef = UNDEF<uint32_t>();
        std::memcpy(&RAM[vsp], &undef, sizeof(uint32_t));
    }
    else if constexpr (size == 16)
    {
        uint16_t undef = UNDEF<uint16_t>();
        std::memcpy(&RAM[vsp], &undef, sizeof(uint16_t));
    }
    else if constexpr (size == 8)
    {
        uint16_t undef = UNDEF<uint16_t>();
        std::memcpy(&RAM[vsp], &undef, sizeof(uint16_t));
    }
    // 5. Update the stack pointer.
    //
    vsp += ((size != 8) ? (size / 8) : ((size / 8) * 2));
}
)

IF_32BIT(
template <size_t size, size_t offset>
INLINE void POP_VREG(size_t &vsp, VirtualRegister &vmreg)
{
    // 1. Fetch and store the value on the virtual register.
    //
    if constexpr (size == 32)
    {
        if constexpr (offset == 0)
        {
            uint32_t value = 0;
            std::memcpy(&value, &RAM[vsp], sizeof(uint32_t));
            vmreg.dword = value;
        }
    }
    else if constexpr (size == 16)
    {
        if constexpr (offset == 0)
        {
            uint16_t value = 0;
            std::memcpy(&value, &RAM[vsp], sizeof(uint16_t));
            vmreg.dword = ((vmreg.dword & 0xFFFF0000) | value);
        }
        else if constexpr (offset == 1)
        {
            uint16_t value = 0;
            std::memcpy(&value, &RAM[vsp], sizeof(uint16_t));
            vmreg.dword = ((vmreg.dword & 0x0000FFFF) | UShl(ZExtTo<uint64_t>(value), 16));
        }
    }
    else if constexpr (size == 8)
    {
        if constexpr (offset == 0)
        {
            uint16_t byte = 0;
            std::memcpy(&byte, &RAM[vsp], sizeof(uint16_t));
            vmreg.byte.b0 = Trunc(byte);
        }
        else if constexpr (offset == 1)
        {
            uint16_t byte = 0;
            std::memcpy(&byte, &RAM[vsp], sizeof(uint16_t));
            vmreg.byte.b1 = Trunc(byte);
        }
        // NOTE: there might be other offsets here, but they were not observed.
        //
    }
    // 4. Clear the value on the stack.
    //
    if constexpr (size == 32)
    {
        uint32_t undef = UNDEF<uint32_t>();
        std::memcpy(&RAM[vsp], &undef, sizeof(uint32_t));
    }
    else if constexpr (size == 16)
    {
        uint16_t undef = UNDEF<uint16_t>();
        std::memcpy(&RAM[vsp], &undef, sizeof(uint16_t));
    }
    else if constexpr (size == 8)
    {
        uint16_t undef = UNDEF<uint16_t>();
        std::memcpy(&RAM[vsp], &undef, sizeof(uint16_t));
    }
    // 5. Update the stack pointer.
    //
    vsp += ((size != 8) ? (size / 8) : ((size / 8) * 2));
}
)

DEFINE_SEMANTIC(POP_VREG_8_0)     = POP_VREG<8, 0>;
DEFINE_SEMANTIC(POP_VREG_8_1)     = POP_VREG<8, 1>;
DEFINE_SEMANTIC(POP_VREG_16_0)    = POP_VREG<16, 0>;
DEFINE_SEMANTIC(POP_VREG_16_2)    = POP_VREG<16, 1>;
DEFINE_SEMANTIC_64(POP_VREG_16_4) = POP_VREG<16, 2>;
DEFINE_SEMANTIC_64(POP_VREG_16_6) = POP_VREG<16, 3>;
DEFINE_SEMANTIC_64(POP_VREG_32_0) = POP_VREG<32, 0>;
DEFINE_SEMANTIC_32(POP_VREG_32)   = POP_VREG<32, 0>;
DEFINE_SEMANTIC_64(POP_VREG_32_4) = POP_VREG<32, 1>;
DEFINE_SEMANTIC_64(POP_VREG_64_0) = POP_VREG<64, 0>;

// Real register push/pop semantic.
//
INLINE void PUSH_REG(size_t &vsp, size_t reg)
{
    // 1. Push the register.
    //
    STACK_PUSH<size_t>(vsp, reg);
}

DEFINE_SEMANTIC_64(PUSH_REG_64) = PUSH_REG;
DEFINE_SEMANTIC_32(PUSH_REG_32) = PUSH_REG;

INLINE void POP_REG(size_t &vsp, size_t &reg)
{
    // 1. Pop the register.
    //
    reg = STACK_POP<size_t>(vsp);
}

DEFINE_SEMANTIC_64(POP_REG_64) = POP_REG;
DEFINE_SEMANTIC_32(POP_REG_32) = POP_REG;

// CPUID semantinc.
//
INLINE void CPUID(size_t &vsp)
{
    // 1. Fetch the operand.
    //
    auto ieax = STACK_POP<uint32_t>(vsp);
    // 2. Call the 'cpuid' intrinsic.
    //
    uint32_t oeax = 0;
    uint32_t oebx = 0;
    uint32_t oecx = 0;
    uint32_t oedx = 0;
    __cpuid(ieax, oeax, oebx, oecx, oedx);
    // 3. Push the 4 affected registers.
    //
    STACK_PUSH<uint32_t>(vsp, oeax);
    STACK_PUSH<uint32_t>(vsp, oebx);
    STACK_PUSH<uint32_t>(vsp, oecx);
    STACK_PUSH<uint32_t>(vsp, oedx);
}

DEFINE_SEMANTIC(CPUID) = CPUID;

// RDTSC semantic.
//
INLINE void RDTSC(size_t &vsp)
{
    // 1. Call the 'rdtsc' instrinsic.
    //
    uint64_t rdtsc = __rdtsc();
    // 2. Split the values.
    //
    uint64_t mask = 0xFFFFFFFF;
    uint32_t eax  = UAnd(rdtsc, mask);
    uint32_t edx  = UAnd(UShr(rdtsc, 32), mask);
    // 3. Push the 2 affected registers.
    //
    STACK_PUSH<uint32_t>(vsp, eax);
    STACK_PUSH<uint32_t>(vsp, edx);
}

DEFINE_SEMANTIC(RDTSC) = RDTSC;

// Arithmetic and logical eflags semantic.
//
INLINE void UPDATE_EFLAGS(size_t &eflags, bool cf, bool pf, bool af, bool zf, bool sf, bool of)
{
    // eflags = 0; // NOTE: vmprotect doesn't actually use the x86 eflags between vmhandlers, the only edge case might be pushf.
    //
    // 1. Update the eflags.
    //
    eflags |= ((eflags & ~(0x001)) | ((size_t)cf << 0));
    eflags |= ((eflags & ~(0x004)) | ((size_t)pf << 2));
    eflags |= ((eflags & ~(0x010)) | ((size_t)af << 4));
    eflags |= ((eflags & ~(0x040)) | ((size_t)zf << 6));
    eflags |= ((eflags & ~(0x080)) | ((size_t)sf << 7));
    eflags |= ((eflags & ~(0x800)) | ((size_t)of << 11));
}

template <typename T>
INLINE CONST bool AF(T lhs, T rhs, T res)
{
    return AuxCarryFlag(lhs, rhs, res);
}

template <typename T>
INLINE CONST bool PF(T res)
{
    return ParityFlag(res);
}

template <typename T>
INLINE CONST bool ZF(T res)
{
    return ZeroFlag(res);
}

template <typename T>
INLINE CONST bool SF(T res)
{
    return SignFlag(res);
}

// ADD semantic.
//
template <typename T>
INLINE CONST bool CF_ADD(T lhs, T rhs, T res)
{
    return Carry<tag_add>::Flag(lhs, rhs, res);
}

template <typename T>
INLINE CONST bool OF_ADD(T lhs, T rhs, T res)
{
    return Overflow<tag_add>::Flag(lhs, rhs, res);
}

template <typename T>
INLINE void ADD_FLAGS(size_t &eflags, T lhs, T rhs, T res)
{
    // 1. Calculate the eflags.
    //
    bool cf = CF_ADD(lhs, rhs, res);
    bool pf = PF(res);
    bool af = AF(lhs, rhs, res);
    bool zf = ZF(res);
    bool sf = SF(res);
    bool of = OF_ADD(lhs, rhs, res);
    // 2. Update the eflags.
    //
    UPDATE_EFLAGS(eflags, cf, pf, af, zf, sf, of);
}

template <typename T>
INLINE void ADD(size_t &vsp)
{
    // 1. Check if it's 'byte' size.
    //
    bool isByte = (sizeof(T) == 1);
    // 2. Initialize the operands.
    //
    T op1 = 0;
    T op2 = 0;
    // 3. Fetch the operands.
    //
    if (isByte)
    {
        op1 = Trunc(STACK_POP<uint16_t>(vsp));
        op2 = Trunc(STACK_POP<uint16_t>(vsp));
    }
    else
    {
        op1 = STACK_POP<T>(vsp);
        op2 = STACK_POP<T>(vsp);
    }
    // 4. Calculate the add.
    //
    T res = UAdd(op1, op2);
    // 5. Calculate the eflags.
    //
    size_t eflags = 0;
    ADD_FLAGS(eflags, op1, op2, res);
    // 6. Save the result.
    //
    if (isByte)
    {
        STACK_PUSH<uint16_t>(vsp, ZExt(res));
    }
    else
    {
        STACK_PUSH<T>(vsp, res);
    }
    // 7. Save the eflags.
    //
    STACK_PUSH<size_t>(vsp, eflags);
}

DEFINE_SEMANTIC_64(ADD_64) = ADD<uint64_t>;
DEFINE_SEMANTIC(ADD_32)    = ADD<uint32_t>;
DEFINE_SEMANTIC(ADD_16)    = ADD<uint16_t>;
DEFINE_SEMANTIC(ADD_8)     = ADD<uint8_t>;

// DIV semantic.
//
INLINE void DIV_FLAGS(size_t &eflags)
{
  // 1. Calculate the eflags.
  //
  bool cf = UNDEF<uint8_t>();
  bool pf = UNDEF<uint8_t>();
  bool af = UNDEF<uint8_t>();
  bool zf = UNDEF<uint8_t>();
  bool sf = UNDEF<uint8_t>();
  bool of = UNDEF<uint8_t>();
  // 2. Update the eflags.
  //
  UPDATE_EFLAGS(eflags, cf, pf, af, zf, sf, of);
}

template <typename T>
INLINE void DIV(size_t &vsp)
{
    // 1. Check if it's 'byte' size.
    //
    bool isByte = (sizeof(T) == 1);
    // 2. Initialize the operands.
    //
    T op1 = 0;
    T op2 = 0;
    T op3 = 0;
    // 3. Fetch the operands.
    //
    if (isByte)
    {
        op1 = Trunc(STACK_POP<uint16_t>(vsp));
        op2 = Trunc(STACK_POP<uint16_t>(vsp));
        op3 = Trunc(STACK_POP<uint16_t>(vsp));
    }
    else
    {
        op1 = STACK_POP<T>(vsp);
        op2 = STACK_POP<T>(vsp);
        op3 = STACK_POP<T>(vsp);
    }
    // 4. Calculate the division.
    //
    auto lhs_lo     = ZExt(op1);
    auto lhs_hi     = ZExt(op2);
    auto rhs        = ZExt(op3);
    auto shift      = ZExt(BitSizeOf(op3));
    auto lhs        = UOr(UShl(lhs_hi, shift), lhs_lo);
    auto quot       = UDiv(lhs, rhs);
    auto rem        = URem(lhs, rhs);
    auto quot_trunc = Trunc(quot);
    auto rem_trunc  = Trunc(rem);
    size_t eflags   = 0;
    // 4.1. Calculate the final values.
    //
    auto rem_final  = quot_trunc;
    auto quot_final = rem_trunc;
    // 4.2. Calculate the eflags.
    //
    DIV_FLAGS(eflags);
    // 4.3. Push the calculated values.
    //
    STACK_PUSH<T>(vsp, rem_final);
    STACK_PUSH<T>(vsp, quot_final);
    STACK_PUSH<size_t>(vsp, eflags);
}

DEFINE_SEMANTIC_64(DIV_64) = DIV<uint64_t>;
DEFINE_SEMANTIC(DIV_32)    = DIV<uint32_t>;
DEFINE_SEMANTIC(DIV_16)    = DIV<uint16_t>;
DEFINE_SEMANTIC(DIV_8)     = DIV<uint8_t>;

// IDIV semantic.
//
INLINE void IDIV_FLAGS(size_t &eflags)
{
  // 1. Calculate the eflags.
  //
  DIV_FLAGS(eflags);
}

template <typename T>
INLINE void IDIV(size_t &vsp)
{
    // 1. Check if it's 'byte' size.
    //
    bool isByte = (sizeof(T) == 1);
    // 2. Initialize the operands.
    //
    T op1 = 0;
    T op2 = 0;
    T op3 = 0;
    // 3. Fetch the operands.
    //
    if (isByte)
    {
        op1 = Trunc(STACK_POP<uint16_t>(vsp));
        op2 = Trunc(STACK_POP<uint16_t>(vsp));
        op3 = Trunc(STACK_POP<uint16_t>(vsp));
    }
    else
    {
        op1 = STACK_POP<T>(vsp);
        op2 = STACK_POP<T>(vsp);
        op3 = STACK_POP<T>(vsp);
    }
    // 4. Calculate the division.
    //
    auto lhs_lo     = ZExt(op1);
    auto lhs_hi     = ZExt(op2);
    auto rhs        = SExt(op3);
    auto shift      = ZExt(BitSizeOf(op3));
    auto lhs        = Signed(UOr(UShl(lhs_hi, shift), lhs_lo));
    auto quot       = SDiv(lhs, rhs);
    auto rem        = SRem(lhs, rhs);
    auto quot_trunc = Trunc(quot);
    auto rem_trunc  = Trunc(rem);
    size_t eflags   = 0;
    // 4.1. Calculate the final values.
    //
    auto rem_final  = Unsigned(quot_trunc);
    auto quot_final = Unsigned(rem_trunc);
    // 4.2. Calculate the eflags.
    //
    IDIV_FLAGS(eflags);
    // 4.3. We are going to push undefined values.
    //
    STACK_PUSH<T>(vsp, rem_final);
    STACK_PUSH<T>(vsp, quot_final);
    STACK_PUSH<size_t>(vsp, eflags);
}

DEFINE_SEMANTIC_64(IDIV_64) = IDIV<uint64_t>;
DEFINE_SEMANTIC(IDIV_32)    = IDIV<uint32_t>;
DEFINE_SEMANTIC(IDIV_16)    = IDIV<uint16_t>;
DEFINE_SEMANTIC(IDIV_8)     = IDIV<uint8_t>;

// MUL semantic.
//
template <typename T, typename R>
INLINE CONST bool CF_MUL(T lhs, T rhs, R res)
{
    return Overflow<tag_mul>::Flag(lhs, rhs, res);
}

template <typename T>
INLINE CONST bool SF_MUL(T lo_res)
{
    return std::is_signed<T>::value ? SignFlag(lo_res) : UNDEF<uint8_t>();
}

template <typename T, typename R>
INLINE void MUL_FLAGS(size_t &eflags, T lhs, T rhs, R res, T lo_res)
{
    // 1. Calculate the eflags.
    //
    bool cf = CF_MUL(lhs, rhs, res);
    bool pf = UNDEF<uint8_t>();
    bool af = UNDEF<uint8_t>();
    bool zf = UNDEF<uint8_t>();
    bool sf = SF_MUL(lo_res);
    bool of = CF_MUL(lhs, rhs, res);
    // 2. Update the eflags.
    //
    UPDATE_EFLAGS(eflags, cf, pf, af, zf, sf, of);
}

template <typename T>
INLINE void MUL(size_t &vsp)
{
    // 1. Check if it's 'byte' size.
    //
    bool isByte = (sizeof(T) == 1);
    // 2. Initialize the operands.
    //
    T op1 = 0;
    T op2 = 0;
    // 3. Fetch the operands.
    //
    if (isByte)
    {
        op1 = Trunc(STACK_POP<uint16_t>(vsp));
        op2 = Trunc(STACK_POP<uint16_t>(vsp));
    }
    else
    {
        op1 = STACK_POP<T>(vsp);
        op2 = STACK_POP<T>(vsp);
    }
    // 4. Force the operands to be unsigned.
    //
    auto lhs = Unsigned(op1);
    auto rhs = Unsigned(op2);
    // 5. Calculate the product.
    //
    auto lhs_wide = ZExt(lhs);
    auto rhs_wide = ZExt(rhs);
    auto res      = UMul(lhs_wide, rhs_wide);
    auto shift    = ZExt(BitSizeOf(lhs));
    auto lo_res   = Trunc(res);
    auto hi_res   = Trunc(UShr(res, shift));
    // 6. Calculate the eflags.
    //
    size_t eflags = 0;
    MUL_FLAGS(eflags, lhs, rhs, res, lo_res);
    // 7. Save the result.
    //
    STACK_PUSH<T>(vsp, lo_res);
    STACK_PUSH<T>(vsp, hi_res);
    STACK_PUSH<size_t>(vsp, eflags);
}

DEFINE_SEMANTIC_64(MUL_64) = MUL<uint64_t>;
DEFINE_SEMANTIC(MUL_32)    = MUL<uint32_t>;
DEFINE_SEMANTIC(MUL_16)    = MUL<uint16_t>;
DEFINE_SEMANTIC(MUL_8)     = MUL<uint8_t>;

// IMUL semantic.
//
template <typename T, typename R>
INLINE void IMUL_FLAGS(size_t &eflags, T lhs, T rhs, R res, T lo_res)
{
    // 1. Calculate the eflags.
    //
    MUL_FLAGS(eflags, lhs, rhs, res, lo_res);
}

template <typename T>
INLINE void IMUL(size_t &vsp)
{
    // 1. Check if it's 'byte' size.
    //
    bool isByte = (sizeof(T) == 1);
    // 2. Initialize the operands.
    //
    T op1 = 0;
    T op2 = 0;
    // 3. Fetch the operands.
    //
    if (isByte)
    {
        op1 = Trunc(STACK_POP<uint16_t>(vsp));
        op2 = Trunc(STACK_POP<uint16_t>(vsp));
    }
    else
    {
        op1 = STACK_POP<T>(vsp);
        op2 = STACK_POP<T>(vsp);
    }
    // 4. Force the operands to be signed.
    //
    auto lhs = Signed(op1);
    auto rhs = Signed(op2);
    // 5. Calculate the product.
    //
    auto lhs_wide = SExt(lhs);
    auto rhs_wide = SExt(rhs);
    auto res      = SMul(lhs_wide, rhs_wide);
    auto shift    = ZExt(BitSizeOf(rhs));
    auto lo_res   = Trunc(res);
    auto hi_res   = Trunc(UShr(Unsigned(res), shift));
    // 6. Calculate the eflags.
    //
    size_t eflags = 0;
    IMUL_FLAGS(eflags, lhs, rhs, res, lo_res);
    // 7. Save the result.
    //
    STACK_PUSH<T>(vsp, lo_res);
    STACK_PUSH<T>(vsp, hi_res);
    STACK_PUSH<size_t>(vsp, eflags);
}

DEFINE_SEMANTIC_64(IMUL_64) = IMUL<uint64_t>;
DEFINE_SEMANTIC(IMUL_32)    = IMUL<uint32_t>;
DEFINE_SEMANTIC(IMUL_16)    = IMUL<uint16_t>;
DEFINE_SEMANTIC(IMUL_8)     = IMUL<uint8_t>;

// NOR semantic.
//
template <typename T>
INLINE void NOR_FLAGS(size_t &eflags, T lhs, T rhs, T res)
{
    // 1. Calculate the eflags.
    //
    bool cf = false;
    bool pf = PF(res);
    bool af = false;
    bool zf = ZF(res);
    bool sf = SF(res);
    bool of = false;
    // 2. Update the eflags.
    //
    UPDATE_EFLAGS(eflags, cf, pf, af, zf, sf, of);
}

template <typename T>
INLINE void NOR(size_t &vsp)
{
    // 1. Check if it's 'byte' size.
    //
    bool isByte = (sizeof(T) == 1);
    // 2. Initialize the operands.
    //
    T op1 = 0;
    T op2 = 0;
    // 3. Fetch the operands.
    //
    if (isByte)
    {
        op1 = Trunc(STACK_POP<uint16_t>(vsp));
        op2 = Trunc(STACK_POP<uint16_t>(vsp));
    }
    else
    {
        op1 = STACK_POP<T>(vsp);
        op2 = STACK_POP<T>(vsp);
    }
    // 4. Calculate the nor.
    //
    T res = UNot(UOr(op1, op2));
    // 5. Calculate the eflags.
    //
    size_t eflags = 0;
    NOR_FLAGS(eflags, op1, op2, res);
    // 6. Save the result.
    //
    if (isByte)
    {
        STACK_PUSH<uint16_t>(vsp, ZExt(res));
    }
    else
    {
        STACK_PUSH<T>(vsp, res);
    }
    // 7. Save the eflags.
    //
    STACK_PUSH<size_t>(vsp, eflags);
}

DEFINE_SEMANTIC_64(NOR_64) = NOR<uint64_t>;
DEFINE_SEMANTIC(NOR_32)    = NOR<uint32_t>;
DEFINE_SEMANTIC(NOR_16)    = NOR<uint16_t>;
DEFINE_SEMANTIC(NOR_8)     = NOR<uint8_t>;

// NAND semantic.
//
template <typename T>
INLINE void NAND_FLAGS(size_t &eflags, T lhs, T rhs, T res)
{
    // 1. Calculate the eflags.
    //
    NOR_FLAGS(eflags, lhs, rhs, res);
}

template <typename T>
INLINE void NAND(size_t &vsp)
{
    // 1. Check if it's 'byte' size.
    //
    bool isByte = (sizeof(T) == 1);
    // 2. Initialize the operands.
    //
    T op1 = 0;
    T op2 = 0;
    // 3. Fetch the operands.
    //
    if (isByte)
    {
        op1 = Trunc(STACK_POP<uint16_t>(vsp));
        op2 = Trunc(STACK_POP<uint16_t>(vsp));
    }
    else
    {
        op1 = STACK_POP<T>(vsp);
        op2 = STACK_POP<T>(vsp);
    }
    // 4. Calculate the nand.
    //
    T res = UNot(UAnd(op1, op2));
    // 5. Calculate the eflags.
    //
    size_t eflags = 0;
    NAND_FLAGS(eflags, op1, op2, res);
    // 6. Save the result.
    //
    if (isByte)
    {
        STACK_PUSH<uint16_t>(vsp, ZExt(res));
    }
    else
    {
        STACK_PUSH<T>(vsp, res);
    }
    // 7. Save the eflags.
    //
    STACK_PUSH<size_t>(vsp, eflags);
}

DEFINE_SEMANTIC_64(NAND_64) = NAND<uint64_t>;
DEFINE_SEMANTIC(NAND_32)    = NAND<uint32_t>;
DEFINE_SEMANTIC(NAND_16)    = NAND<uint16_t>;
DEFINE_SEMANTIC(NAND_8)     = NAND<uint8_t>;

// SHL semantic.
//
template <typename T>
INLINE CONST bool OF_SHL(T val, T res)
{
    return BXor(SignFlag(val), SignFlag(res));
}

template <typename T>
INLINE CONST bool CF_SHL(T op1, T op2, T res)
{
    T long_mask       = 0x3F;
    T short_mask      = 0x1F;
    auto op_size      = BitSizeOf(op1);
    auto shift_mask   = Select(UCmpEq(op_size, 64), long_mask, short_mask);
    auto masked_shift = UAnd(op2, shift_mask);

    if (UCmpEq(masked_shift, 1))
    {
        return SF(op1);
    }
    else if (UCmpLt(masked_shift, op_size))
    {
        return SF(res);
    }
    return UNDEF<uint8_t>();
}

template <typename T>
INLINE void SHL(size_t &vsp)
{
    // 1. Check if it's 'byte' size.
    //
    bool isByte = (sizeof(T) == 1);
    // 2. Initialize the operands.
    //
    T op1 = 0;
    T op2 = 0;
    // 3. Fetch the operands.
    //
    if (isByte)
    {
        op1 = Trunc(STACK_POP<uint16_t>(vsp));
        op2 = Trunc(STACK_POP<uint16_t>(vsp));
    }
    else
    {
        op1 = STACK_POP<T>(vsp);
        op2 = STACK_POP<uint16_t>(vsp);
    }
    // 4. Calculate the shift.
    //
    auto res = op1 << op2;
    // 5. Calculate the eflags.
    //
    size_t eflags = 0;
    bool cf = CF_SHL<T>(op1, op2, res);
    bool pf = PF(res);
    bool af = UNDEF<uint8_t>();
    bool zf = ZF(res);
    bool sf = SF(res);
    bool of = OF_SHL<T>(op1, res);
    // Update the eflags.
    //
    UPDATE_EFLAGS(eflags, cf, pf, af, zf, sf, of);
    // 6. Save the result.
    //
    if (isByte)
    {
        STACK_PUSH<uint16_t>(vsp, ZExt(res));
    }
    else
    {
        STACK_PUSH<T>(vsp, res);
    }
    STACK_PUSH<size_t>(vsp, eflags);
}

DEFINE_SEMANTIC_64(SHL_64) = SHL<uint64_t>;
DEFINE_SEMANTIC(SHL_32)    = SHL<uint32_t>;
DEFINE_SEMANTIC(SHL_16)    = SHL<uint16_t>;
DEFINE_SEMANTIC(SHL_8)     = SHL<uint8_t>;

// SHR semantic.
//
template <typename T>
INLINE CONST bool CF_SHR(T op1, T op2, T res)
{
    T long_mask       = 0x3F;
    T short_mask      = 0x1F;
    auto op_size      = BitSizeOf(op1);
    auto shift_mask   = Select(UCmpEq(op_size, 64), long_mask, short_mask);
    auto masked_shift = UAnd(op2, shift_mask);

    if (UCmpEq(masked_shift, 1))
    {
        return UCmpEq(UAnd(op1, 1), 1);
    }
    else if (UCmpLt(masked_shift, op_size))
    {
        return UCmpEq(UAnd(res, 1), 1);
    }
    return UNDEF<uint8_t>();
}

template <typename T>
INLINE CONST bool OF_SHR(T val)
{
    return SF(val);
}

template <typename T>
INLINE void SHR(size_t &vsp)
{
    // 1. Check if it's 'byte' size.
    //
    bool isByte = (sizeof(T) == 1);
    // 2. Initialize the operands.
    //
    T op1 = 0;
    T op2 = 0;
    // 3. Fetch the operands.
    //
    if (isByte)
    {
        op1 = Trunc(STACK_POP<uint16_t>(vsp));
        op2 = Trunc(STACK_POP<uint16_t>(vsp));
    }
    else
    {
        op1 = STACK_POP<T>(vsp);
        op2 = STACK_POP<uint16_t>(vsp);
    }
    // 4. Calculate shift.
    //
    auto res = op1 >> op2;
    // 5. Calculate the eflags.
    //
    size_t eflags = 0;
    bool cf = CF_SHR<T>(op1, op2, res);
    bool pf = PF(res);
    bool af = UNDEF<uint8_t>();
    bool zf = ZF(res);
    bool sf = false;
    bool of = OF_SHR(op1);
    UPDATE_EFLAGS(eflags, cf, pf, af, zf, sf, of);
    // 6. Save the result.
    //
    if (isByte)
    {
        STACK_PUSH<uint16_t>(vsp, ZExt(res));
    }
    else
    {
        STACK_PUSH<T>(vsp, res);
    }
    STACK_PUSH<size_t>(vsp, eflags);
}

DEFINE_SEMANTIC_64(SHR_64) = SHR<uint64_t>;
DEFINE_SEMANTIC(SHR_32)    = SHR<uint32_t>;
DEFINE_SEMANTIC(SHR_16)    = SHR<uint16_t>;
DEFINE_SEMANTIC(SHR_8)     = SHR<uint8_t>;

// SHLD semantic.
//
template <typename T>
INLINE CONST bool CF_SHLD(T val, T masked_shift)
{
    return SHLDCarryFlag(val, masked_shift);
}

template <typename T>
INLINE CONST bool OF_SHLD(T val, T res)
{
    return BXor(SignFlag(val), SignFlag(res));
}

template <typename T>
INLINE void SHLD(size_t &vsp)
{
    // 1. Fetch the operands.
    //
    T val1  = STACK_POP<T>(vsp);
    T val2  = STACK_POP<T>(vsp);
    T shift = STACK_POP<uint16_t>(vsp);
    // 2. Calculate the shift left.
    //
    T long_mask       = 0x3F;
    T short_mask      = 0x1F;
    auto op_size      = BitSizeOf(val1);
    auto shift_mask   = Select(UCmpEq(op_size, 64), long_mask, short_mask);
    auto masked_shift = UAnd(shift, shift_mask);
    // Execute the real shift.
    //
    auto left  = UShl(val1, masked_shift);
    auto right = UShr(val2, USub(op_size, masked_shift));
    auto res   = UOr(left, right);
    // Calculate the eflags.
    //
    size_t eflags = 0;
    bool cf = CF_SHLD(val1, masked_shift);
    bool pf = PF(res);
    bool af = UNDEF<uint8_t>();
    bool zf = ZF(res);
    bool sf = SF(res);
    bool of = OF_SHLD(val1, res);
    // Update the eflags.
    //
    UPDATE_EFLAGS(eflags, cf, pf, af, zf, sf, of);
    // Save the result.
    //
    STACK_PUSH<T>(vsp, res);
    STACK_PUSH<size_t>(vsp, eflags);
}

DEFINE_SEMANTIC_64(SHLD_64) = SHLD<uint64_t>;
DEFINE_SEMANTIC(SHLD_32)    = SHLD<uint32_t>;
DEFINE_SEMANTIC(SHLD_16)    = SHLD<uint16_t>;
DEFINE_SEMANTIC(SHLD_8)     = SHLD<uint8_t>;

// SHRD semantic.
//
template <typename T>
INLINE CONST bool CF_SHRD(T val, T masked_shift)
{
    return SHRDCarryFlag(val, masked_shift);
}

template <typename T>
INLINE CONST bool OF_SHRD(T val, T res)
{
    return BXor(SignFlag(val), SignFlag(res));
}

template <typename T>
INLINE void SHRD(size_t &vsp)
{
    // 1. Fetch the operands.
    //
    T val1  = STACK_POP<T>(vsp);
    T val2  = STACK_POP<T>(vsp);
    T shift = STACK_POP<uint16_t>(vsp);
    // 2. Calculate the shift right.
    //
    T long_mask       = 0x3F;
    T short_mask      = 0x1F;
    auto op_size      = BitSizeOf(val1);
    auto shift_mask   = Select(UCmpEq(op_size, 64), long_mask, short_mask);
    auto masked_shift = UAnd(shift, shift_mask);
    // Execute the real shift.
    //
    auto left  = UShl(val2, USub(op_size, masked_shift));
    auto right = UShr(val1, masked_shift);
    auto res   = UOr(left, right);
    // Calculate the eflags.
    //
    size_t eflags = 0;
    bool cf = CF_SHRD(val1, masked_shift);
    bool pf = PF(res);
    bool af = UNDEF<uint8_t>();
    bool zf = ZF(res);
    bool sf = SF(res);
    bool of = OF_SHRD(val1, res);
    // Update the eflags.
    //
    UPDATE_EFLAGS(eflags, cf, pf, af, zf, sf, of);
    // Save the result.
    //
    STACK_PUSH<T>(vsp, res);
    STACK_PUSH<size_t>(vsp, eflags);
}

DEFINE_SEMANTIC_64(SHRD_64) = SHRD<uint64_t>;
DEFINE_SEMANTIC(SHRD_32)    = SHRD<uint32_t>;
DEFINE_SEMANTIC(SHRD_16)    = SHRD<uint16_t>;
DEFINE_SEMANTIC(SHRD_8)     = SHRD<uint8_t>;

// JUMP semantic.
//
INLINE void JMP(size_t &vsp, size_t &vip)
{
    vip = STACK_POP<size_t>(vsp);
}

INLINE void JCC_DEC(size_t &vsp, size_t &vip)
{
    vip = STACK_POP<size_t>(vsp) - 4;
}

INLINE void JCC_INC(size_t &vsp, size_t &vip)
{
    vip = STACK_POP<size_t>(vsp) + 4;
}

DEFINE_SEMANTIC(JCC_INC) = JCC_INC;
DEFINE_SEMANTIC(JCC_DEC) = JCC_DEC;
DEFINE_SEMANTIC(JMP)     = JMP;
DEFINE_SEMANTIC(RET)     = JMP;

// Helper function to keep the PC value.
//
extern "C" CONST __attribute__((noduplicate)) __attribute__((nomerge)) size_t KeepReturn(size_t ret0, size_t ret1);

extern "C" void retainPointers()
{
    RAM[0] = KeepReturn(0, 0);
    GS[0]  = 0;
    FS[0]  = 0;
}


IF_64BIT(
extern "C" CONST __attribute__((noduplicate)) __attribute__((nomerge)) size_t ExternalFunction(size_t rcx, size_t rdx, size_t r8, size_t r9);
)

IF_64BIT(
INLINE extern "C" size_t ExternalFunctionRetain(size_t rcx, size_t rdx, size_t r8, size_t r9)
{
    return ExternalFunction(rcx, rdx, r8, r9);
})

IF_64BIT(
extern "C" size_t VirtualStub(rptr rax, rptr rbx, rptr rcx, rptr rdx, rptr rsi, rptr rdi, rptr rbp, rptr rsp, rptr r8, rptr r9,  rptr r10, rptr r11, rptr r12, rptr r13, rptr r14, rptr r15, rptr eflags, rptr vsp, rptr vip, VirtualRegister *__restrict__ vmregs);
)

IF_64BIT(
INLINE extern "C" size_t VirtualStubEmpty(rptr rax, rptr rbx, rptr rcx, rptr rdx, rptr rsi, rptr rdi, rptr rbp, rptr rsp, rptr r8, rptr r9,  rptr r10, rptr r11, rptr r12, rptr r13, rptr r14, rptr r15, rptr eflags, rptr vsp, rptr vip, VirtualRegister *__restrict__ vmregs)
{
    return 0;
})

IF_64BIT(
extern "C" INLINE size_t VirtualFunction(rptr rax, rptr rbx, rptr rcx, rptr rdx, rptr rsi, rptr rdi, rptr rbp, rptr rsp, rptr r8, rptr r9,  rptr r10, rptr r11, rptr r12, rptr r13, rptr r14, rptr r15, rptr eflags)
    {
    VirtualRegister vmregs[30] = { 0 };
    size_t vip = 0;
    vip = VirtualStub(rax, rbx, rcx, rdx, rsi, rdi, rbp, rsp, r8, r9, r10, r11, r12, r13, r14, r15, eflags, rsp, vip, vmregs);
    eflags = UNDEF<size_t>();
    return vip;
})

IF_64BIT(
extern "C" size_t SlicePC(size_t rax, size_t rbx, size_t rcx, size_t rdx, size_t rsi, size_t rdi, size_t rbp, size_t rsp, size_t r8, size_t r9, size_t r10, size_t r11, size_t r12, size_t r13, size_t r14, size_t r15, size_t eflags)
{
  VirtualRegister vmregs[30] = { 0 };
  size_t vip = 0;
  size_t vsp = rsp;
  vip = VirtualStub(rax, rbx, rcx, rdx, rsi, rdi, rbp, rsp, r8, r9, r10, r11, r12, r13, r14, r15, eflags, vsp, vip, vmregs);
  eflags = 0;
  return vip;
})

IF_32BIT(
extern "C" CONST __attribute__((noduplicate)) __attribute__((nomerge)) size_t ExternalFunction(rptr eax, rptr ebx, rptr ecx, rptr edx, rptr esi, rptr edi, rptr ebp);
)

IF_32BIT(
INLINE extern "C" size_t ExternalFunctionRetain(rptr eax, rptr ebx, rptr ecx, rptr edx, rptr esi, rptr edi, rptr ebp)
{
    return ExternalFunction(eax, ebx, ecx, edx, esi, edi, ebp);
})

IF_32BIT(
extern "C" size_t VirtualStub(rptr eax, rptr ebx, rptr ecx, rptr edx, rptr esi, rptr edi, rptr ebp, rptr esp, rptr eip, rptr eflags, rptr vsp, rptr vip, VirtualRegister *__restrict__ vmregs);
)

IF_32BIT(
INLINE extern "C" size_t VirtualStubEmpty(rptr eax, rptr ebx, rptr ecx, rptr edx, rptr esi, rptr edi, rptr ebp, rptr esp, rptr eip, rptr eflags, rptr vsp, rptr vip, VirtualRegister *__restrict__ vmregs)
{
    return 0;
})

IF_32BIT(
extern "C" INLINE size_t VirtualFunction(rptr eax, rptr ebx, rptr ecx, rptr edx, rptr esi, rptr edi, rptr ebp, rptr esp, rptr eip, rptr eflags)
{
  VirtualRegister vmregs[30] = { 0 };
  size_t vip = 0;
  size_t vsp = esp;
  vip = VirtualStub(eax, ebx, ecx, edx, esi, edi, ebp, esp, eip, eflags, vsp, vip, vmregs);
  esp = vsp;
  eflags = UNDEF<size_t>();
  return vip;
})

IF_32BIT(
extern "C" size_t SlicePC(size_t eax, size_t ebx, size_t ecx, size_t edx, size_t esi, size_t edi, size_t ebp, size_t esp, size_t eip, size_t eflags)
{
    VirtualRegister vmregs[30] = { 0 };
    size_t vsp = esp;
    size_t vip = 0;
    vip = VirtualStub(eax, ebx, ecx, edx, esi, edi, ebp, esp, eip, eflags, vsp, vip, vmregs);
    eflags = 0;
    return vip;
})

```

`shell.nix`:

```nix
{ pkgs ? import <unstable> {} }:
let
  stdenv = pkgs.llvmPackages_15.stdenv;

  triton = stdenv.mkDerivation rec {
    version = "dev-v1.0";
    name = "triton-${version}";

    src = pkgs.fetchFromGitHub {
      owner = "JonathanSalwan";
      repo = "Triton";
      rev = "6095a21c332caa3435e5ce9a88f544f2b9c3be5b";
      sha256 = "sha256-beexdXd48i3OHPOPOOJ59go1+UH1fqL7JRjwl14bKRQ=";
    };

    cmakeFlags = [
      "-DBOOST_INTERFACE=OFF"
      "-DBUILD_EXAMPLES=OFF"
      "-DENABLE_TEST=OFF"
      "-DPYTHON_BINDINGS=OFF"
      "-DLLVM_INTERFACE=ON"
    ];

    nativeBuildInputs = [
      pkgs.cmake
    ];

    buildInputs = [
      pkgs.capstone
      pkgs.llvm_15
      pkgs.z3
    ];
  };

in rec {
  titan = stdenv.mkDerivation {
    name = "titan";

    nativeBuildInputs = [
      pkgs.cmake
      pkgs.ninja
      pkgs.clang_15
      pkgs.graphviz
    ];

    buildInputs = [
      pkgs.range-v3
      pkgs.fmt
      pkgs.llvm_15
      triton
    ];
  };
}

```

`src/asserts.hpp`:

```hpp
#pragma once
#include <stdexcept>

inline static constexpr void abort_if(bool condition, const char* string)
{
    if (condition)
    {
        throw std::logic_error{ string };
    }
}

#define fassert__stringify(x) #x
#define fassert__istringify(x) fassert__stringify(x)
#define fassert(...) abort_if(!bool(__VA_ARGS__), fassert__stringify(__VA_ARGS__) " at " __FILE__ ":" fassert__istringify(__LINE__))

```

`src/binary.cpp`:

```cpp
#include "binary.hpp"
#include "logger.hpp"

#include <llvm/Support/CommandLine.h>
#include <llvm/Object/ObjectFile.h>
#include <llvm/Support/Error.h>

llvm::cl::opt<std::string> binarypath("b",
    llvm::cl::desc("Path to the target Binary"),
    llvm::cl::value_desc("Binary"),
    llvm::cl::Required);

Binary::Binary()
{
    auto object_or_err = llvm::object::ObjectFile::createObjectFile(binarypath);

    if (object_or_err.takeError())
    {
        logger::error("Binary::Binary: Failed to create object file.");
    }

    std::tie(object, memory) = object_or_err->takeBinary();
}

std::optional<llvm::object::SectionRef> Binary::get_section(uint64_t address) const noexcept
{
    for (const auto& section : object->sections())
    {
        if (address >= section.getAddress() && address < section.getAddress() + section.getSize())
        {
            return section;
        }
    }
    return std::nullopt;
}

std::vector<uint8_t> Binary::get_bytes(uint64_t address, size_t size) const noexcept
{
    std::vector<uint8_t> raw;
    if (auto section = get_section(address))
    {
        if (auto offset = address - section->getAddress(); section->getSize() >= offset + size)
        {
            if (auto contents = section->getContents())
            {
                for (auto byte : contents->substr(offset, size))
                {
                    raw.push_back(byte);
                }
            }
            else
            {
                logger::info("Binary::get_bytes: Failed to read {} bytes from 0x{:x}.", size, address);
            }
        }
        else
        {
            logger::info("Binary::get_bytes: No offset within section for 0x{:x}:{} was found.", address, size);
        }
    }
    return raw;
}

bool Binary::is_x64() const noexcept
{
    return object->getArch() == llvm::Triple::ArchType::x86_64;
}

```

`src/binary.hpp`:

```hpp
#pragma once
#include <llvm/ADT/StringRef.h>
#include <llvm/Object/ObjectFile.h>
#include <llvm/Support/MemoryBuffer.h>

#include <memory>
#include <string>
#include <cstdint>
#include <optional>

struct Binary
{
    Binary();

    auto get_section(uint64_t address)            const noexcept -> std::optional<llvm::object::SectionRef>;
    auto get_bytes(uint64_t address, size_t size) const noexcept -> std::vector<uint8_t>;

    template<typename T>
    std::vector<uint8_t> get_bytes(uint64_t address) const noexcept
    {
        return get_bytes(address, sizeof(T));
    }

    bool is_x64() const noexcept;

    auto begin() { return object->section_begin(); }
    auto end()   { return object->section_end();   }

private:
    std::unique_ptr<llvm::object::ObjectFile> object;
    std::unique_ptr<llvm::MemoryBuffer> memory;
};

```

`src/emulator.cpp`:

```cpp
#include "emulator.hpp"
#include "logger.hpp"

#include <triton/x8664Cpu.hpp>
#include <triton/x86Cpu.hpp>

using namespace triton;
using namespace triton::arch;
using namespace triton::arch::x86;

Emulator::Emulator(triton::arch::architecture_e arch) noexcept
    : Context(arch)
    , image{ std::make_shared<Binary>() }
{
    setMode(modes::MEMORY_ARRAY, false);
    setMode(modes::ALIGNED_MEMORY, true);
    setMode(modes::CONSTANT_FOLDING, true);
    setMode(modes::AST_OPTIMIZATIONS, true);
    setMode(modes::PC_TRACKING_SYMBOLIC, false);
    setMode(modes::TAINT_THROUGH_POINTERS, false);
    setMode(modes::SYMBOLIZE_INDEX_ROTATION, false);

    concretizeAllMemory();
    concretizeAllRegister();

    auto get_memory_cb = [this](triton::Context& context, const triton::arch::MemoryAccess& memory)
    {
        if (!context.isConcreteMemoryValueDefined(memory.getAddress(), memory.getSize()))
        {
            context.setConcreteMemoryAreaValue(memory.getAddress(), this->image->get_bytes(memory.getAddress(), memory.getSize()));
        }
    };

    addCallback(
        triton::callbacks::callback_e::GET_CONCRETE_MEMORY_VALUE,
        triton::callbacks::getConcreteMemoryValueCallback{ get_memory_cb, &get_memory_cb }
    );
}

Emulator::Emulator(Emulator const& other) noexcept
    : Emulator(other.getArchitecture())
{
    for (const auto& [reg_e, reg] : other.getAllRegisters())
        setConcreteRegisterValue(reg, other.getConcreteRegisterValue(reg));

    for (const auto& [addr, value] : other.getConcreteMemory())
        setConcreteMemoryValue(addr, value);
    image = other.image;
}

uint64_t Emulator::read(const triton::arch::Register& reg) const noexcept
{
    return static_cast<uint64_t>(getConcreteRegisterValue(reg));
}

void Emulator::write(const triton::arch::Register& reg, uint64_t value) noexcept
{
    setConcreteRegisterValue(reg, value);
}

uint64_t Emulator::rip() const noexcept
{
    return read(rip_register());
}

uint64_t Emulator::rsp() const noexcept
{
    return read(rsp_register());
}

const triton::arch::Register& Emulator::rip_register() const noexcept
{
    return getRegister(getArchitecture() == arch::ARCH_X86_64 ? "rip" : "eip");
}

const triton::arch::Register& Emulator::rsp_register() const noexcept
{
    return getRegister(getArchitecture() == arch::ARCH_X86_64 ? "rsp" : "esp");
}

uint64_t Emulator::ptrsize() const noexcept
{
    return getArchitecture() == ARCH_X86_64 ? 8 : 4;
}

std::set<triton::arch::Register> Emulator::regs() const noexcept
{
    if (getArchitecture() == triton::arch::ARCH_X86_64)
        return {
            getRegister("rax"),
            getRegister("rbx"),
            getRegister("rcx"),
            getRegister("rdx"),
            getRegister("rdi"),
            getRegister("rsi"),
            getRegister("rsp"),
            getRegister("rbp"),
            getRegister("r8"),
            getRegister("r9"),
            getRegister("r10"),
            getRegister("r11"),
            getRegister("r12"),
            getRegister("r13"),
            getRegister("r14"),
            getRegister("r15"),
        };
    return {
        getRegister("eax"),
        getRegister("ebx"),
        getRegister("ecx"),
        getRegister("edx"),
        getRegister("edi"),
        getRegister("esi"),
        getRegister("esp"),
        getRegister("ebp"),
    };
}

triton::arch::Instruction Emulator::disassemble() const noexcept
{
    auto curr_pc = rip();
    auto bytes   = getConcreteMemoryAreaValue(curr_pc, 16);

    Instruction insn(curr_pc, bytes.data(), bytes.size());
    disassembly(insn);
    return insn;
}

triton::arch::Instruction Emulator::single_step()
{
    auto insn = disassemble();
    execute(insn);
    return insn;
}

void Emulator::execute(triton::arch::Instruction& insn)
{
    if (buildSemantics(insn) != triton::arch::NO_FAULT)
    {
        logger::error("Emulator::execute: Failed to execute instruction at 0x{:x}.", rip());
    }
}

```

`src/emulator.hpp`:

```hpp
#pragma once

#include "binary.hpp"

#include <functional>
#include <optional>

#include <triton/context.hpp>
#include <triton/basicBlock.hpp>
#include <triton/x86Specifications.hpp>

struct Emulator : public triton::Context
{
    explicit Emulator(triton::arch::architecture_e arch) noexcept;

    Emulator(Emulator const& other) noexcept;

    uint64_t ptrsize() const noexcept;
    // Most used registers getters.
    //
    uint64_t rip() const noexcept;
    uint64_t rsp() const noexcept;

    const triton::arch::Register& rip_register() const noexcept;
    const triton::arch::Register& rsp_register() const noexcept;

    std::set<triton::arch::Register> regs() const noexcept;

    uint64_t read(const triton::arch::Register& reg) const noexcept;
    template<typename T>
    T read(uint64_t address) const noexcept
    {
        return static_cast<T>(getConcreteMemoryValue({ address, sizeof(T) }));
    }
    template<typename T>
    T read(const triton::arch::MemoryAccess& memory) const noexcept
    {
        return static_cast<T>(getConcreteMemoryValue(memory));
    }

    void write(const triton::arch::Register& reg, uint64_t value) noexcept;
    template<typename T>
    void write(uint64_t address, T value) noexcept
    {
        setConcreteMemoryValue({ address, sizeof(T) }, value);
    }
    template<typename T>
    void write(const triton::arch::MemoryAccess& memory, T value) const noexcept
    {
        return static_cast<T>(setConcreteMemoryValue(memory, value));
    }

    triton::arch::Instruction disassemble() const noexcept;
    triton::arch::Instruction single_step();

    void execute(triton::arch::Instruction& insn);

protected:
    std::shared_ptr<Binary> image;
};

```

`src/explorer.cpp`:

```cpp
#include "explorer.hpp"
#include "il/optimizer.hpp"
#include "il/solver.hpp"
#include "asserts.hpp"
#include "utils.hpp"

static constexpr auto stack_base = 0x10000;

Explorer::Explorer(std::shared_ptr<Lifter> lifter, std::shared_ptr<Tracer> tracer)
    : lifter(lifter), tracer(tracer), block(nullptr), terminate(false)
{
}

std::unique_ptr<vm::Routine> Explorer::explore(uint64_t address)
{
    tracer->write(tracer->rip_register(), address);
    tracer->write(tracer->rsp_register(), stack_base);

    block = vm::Routine::begin(address);

    std::visit(*this, tracer->step(step_t::stop_before_branch));

    worklist.push(address);
    snapshots.emplace(address, std::move(tracer));

    while (!worklist.empty())
    {
        address = worklist.top(); worklist.pop();

        if (explored.count(address))
        {
            logger::warn("block 0x{:x} already explored.", address);
            continue;
        }
        explored.insert(address);

        block  = block->owner->blocks.at(address);
        tracer = snapshots.at(address);

        if (block->lifted != nullptr)
        {
            reprove_block();
            continue;
        }

        logger::debug("exploring 0x{:x}", address);

        while (!terminate)
        {
            // logger::info("execute: 0x{:x}", tracer->rip());
            // Process instruction.
            //
            std::visit(*this, tracer->step(step_t::stop_before_branch));
        }
        terminate = false;

        for (const auto& reprove : get_reprove_blocks())
        {
            logger::info("\treprove -> 0x{:x}", reprove);
            worklist.push(reprove);
            explored.erase(reprove);
        }
    }
    return std::unique_ptr<vm::Routine>{ block->owner };
}

void Explorer::operator()(vm::Add&& insn)
{
    logger::info("{:<5} {:<2}", "add", insn.size());
    block->add(std::move(insn));
}

void Explorer::operator()(vm::Shl&& insn)
{
    logger::info("{:<5} {:<2}", "shl", insn.size());
    block->add(std::move(insn));
}

void Explorer::operator()(vm::Shr&& insn)
{
    logger::info("{:<5} {:<2}", "shr", insn.size());
    block->add(std::move(insn));
}

void Explorer::operator()(vm::Ldr&& insn)
{
    logger::info("{:<5} {:<2}", "ldr", insn.size());
    block->add(std::move(insn));
}

void Explorer::operator()(vm::Str&& insn)
{
    logger::info("{:<5} {:<2}", "str", insn.size());
    block->add(std::move(insn));
}

void Explorer::operator()(vm::Nor&& insn)
{
    logger::info("{:<5} {:<2}", "nor", insn.size());
    block->add(std::move(insn));
}

void Explorer::operator()(vm::Nand&& insn)
{
    logger::info("{:<5} {:<2}", "nand", insn.size());
    block->add(std::move(insn));
}

void Explorer::operator()(vm::Shrd&& insn)
{
    logger::info("{:<5} {:<2}", "shrd", insn.size());
    block->add(std::move(insn));
}

void Explorer::operator()(vm::Shld&& insn)
{
    logger::info("{:<5} {:<2}", "shld", insn.size());
    block->add(std::move(insn));
}

void Explorer::operator()(vm::Push&& insn)
{
    logger::info("{:<5} {:<2} {}", "push", insn.size(), insn.op().to_string());
    block->add(std::move(insn));
}

void Explorer::operator()(vm::Pop&& insn)
{
    logger::info("{:<5} {:<2} {}", "pop", insn.size(), insn.op().to_string());
    block->add(std::move(insn));
}

void Explorer::operator()(vm::Jmp&& insn)
{
    logger::info("jmp");
    block->lifted = lifter->lift_basic_block(block);
    il::optimize_block_function(block->lifted);
    // Execute branch instruction.
    //
    tracer->step(step_t::execute_branch);
    // Fork block and continue executing from new one.
    //
    auto vip = tracer->vip();
    block->fork(vip);
    worklist.push(vip);
    snapshots.emplace(vip, tracer->fork());
    // Terminate current block.
    //
    terminate = true;
}

void Explorer::operator()(vm::Ret&& insn)
{
    block->add(std::move(insn));
    // Terminate current block.
    //
    terminate = true;
}

void Explorer::operator()(vm::Jcc&& insn)
{
    logger::info("jcc {}", insn.direction() == vm::jcc_e::up ? "up" : "down");
    block->add(insn);
    // Lift basic block.
    //
    block->lifted = lifter->lift_basic_block(block);
    il::optimize_block_function(block->lifted);
    // Extract targets.
    //
    auto slice = lifter->build_function(block->owner, block->vip());
    il::optimize_block_function(slice);

    auto ret = lifter->get_return_args(slice);

    for (const auto target : il::get_possible_targets(ret.program_counter()))
    {
        logger::info("\tjcc -> 0x{:x}", target);

        auto fork = tracer->fork();
        fork->write(fork->vsp(), target - (insn.direction() == vm::jcc_e::up ? 1 : -1) * 4);
        // Execute branch instruction.
        //
        fork->step(step_t::execute_branch);

        block->fork(target);
        worklist.push(target);
        snapshots.insert({ target, std::move(fork) });
    }
    // Terminate current block.
    //
    terminate = true;

    slice->eraseFromParent();
}

void Explorer::operator()(vm::Exit&& insn)
{
    for (const auto& reg : insn.regs())
    {
        logger::info("{:<5} {:<2} {}", "pop", reg.size(), reg.op().to_string());
    }
    logger::info("ret");
    // Add instructions to the block.
    //
    block->add(std::move(insn));
    block->add(vm::Ret());
    // Lift basic block.
    //
    block->lifted = lifter->lift_basic_block(block);
    il::optimize_block_function(block->lifted);

    auto slice = lifter->build_function(block->owner, block->vip());
    il::optimize_block_function(slice);

    auto args = lifter->get_return_args(slice);

    args.program_counter()->dump();
    args.return_address()->dump();
    // NOTE: This is not tested and probably wrong.
    //
    if (auto load = llvm::dyn_cast<llvm::LoadInst>(args.program_counter()))
    {
        if (auto gep = llvm::dyn_cast<llvm::GEPOperator>(load->getPointerOperand()))
        {
            if (auto cint = llvm::dyn_cast<llvm::ConstantInt>(gep->getOperand(gep->getNumOperands() - 1)))
            {
                lifter->create_external_call(block->lifted, fmt::format("External.0x{:x}", cint->getLimitedValue()));
                il::optimize_block_function(block->lifted);
            }
        }
    }
    else if (auto cint = llvm::dyn_cast<llvm::ConstantInt>(args.program_counter()))
    {
        lifter->create_external_call(block->lifted, fmt::format("External.0x{:x}", cint->getLimitedValue()));
        il::optimize_block_function(block->lifted);
    }

    if (auto cint = llvm::dyn_cast<llvm::ConstantInt>(args.return_address()))
    {
        auto address = cint->getLimitedValue();
        logger::info("Continue vm execution from 0x{:x}", address);
        tracer = std::make_shared<Tracer>(tracer->getArchitecture());
        tracer->write(tracer->rip_register(), address);
        tracer->write(tracer->rsp_register(), stack_base);

        block->fork(address);
        worklist.push(address);
        snapshots.insert({ address, std::move(tracer) });
    }
    // Terminate current block.
    //
    terminate = true;

    slice->eraseFromParent();
}

void Explorer::operator()(vm::Enter&& insn)
{
    for (const auto& reg : insn.regs())
    {
        logger::info("{:<5} {:<2} {}", "push", reg.size(), reg.op().to_string());
    }
    // Add instructions to the block.
    //
    block->add(std::move(insn));
}

void Explorer::reprove_block()
{
    auto slice = lifter->build_function(block->owner, block->vip());
    il::optimize_block_function(slice);

    auto ret = lifter->get_return_args(slice);

    for (const auto target : il::get_possible_targets(ret.program_counter()))
    {
        if (!block->owner->contains(target))
        {
            logger::info("\tfound new branch: 0x{:x}", target);

            auto fork = tracer->fork();
            auto insn = std::get<vm::Jcc>(tracer->step(step_t::stop_before_branch));

            fork->write(fork->vsp(), target - (insn.direction() == vm::jcc_e::up ? 1 : -1) * 4);
            // Execute branch instruction.
            //
            fork->step(step_t::execute_branch);

            block->fork(target);
            worklist.push(target);
            snapshots.insert({ target, std::move(fork) });
        }
    }

    slice->eraseFromParent();
}

std::set<uint64_t> Explorer::get_reprove_blocks()
{
    std::set<uint64_t> reprove;

    auto fill = [&reprove](const vm::BasicBlock* block, auto&& func) -> void
    {
        for (const auto& child : block->next)
        {
            if (!reprove.contains(child->vip()) && child->next.size() != 2 && child->flow() == vm::flow_t::conditional)
            {
                reprove.insert(child->vip());
                func(child, func);
            }
        }
    };
    fill(block, fill);
    return reprove;
}

```

`src/explorer.hpp`:

```hpp
#pragma once

#include "vm/routine.hpp"

#include "lifter.hpp"
#include "tracer.hpp"

#include <stack>

struct Explorer
{
    Explorer(std::shared_ptr<Lifter> lifter, std::shared_ptr<Tracer> tracer);

    std::unique_ptr<vm::Routine> explore(uint64_t address);

    void operator()(vm::Add&&);
    void operator()(vm::Shl&&);
    void operator()(vm::Shr&&);
    void operator()(vm::Ldr&&);
    void operator()(vm::Str&&);
    void operator()(vm::Nor&&);
    void operator()(vm::Nand&&);
    void operator()(vm::Shld&&);
    void operator()(vm::Shrd&&);
    void operator()(vm::Push&&);
    void operator()(vm::Pop&&);
    void operator()(vm::Jmp&&);
    void operator()(vm::Ret&&);
    void operator()(vm::Jcc&&);
    void operator()(vm::Exit&&);
    void operator()(vm::Enter&&);

private:
    void reprove_block();

    std::set<uint64_t> get_reprove_blocks();

    // LLVM Lifter instance.
    //
    std::shared_ptr<Lifter> lifter;

    // Emulator that is currently active.
    //
    std::shared_ptr<Tracer> tracer;

    // List of blocks to explore.
    //
    std::stack<uint64_t> worklist;

    // List of blocks already explored.
    //
    std::set<uint64_t> explored;

    // Saved snapshots for every basic block.
    //
    std::map<uint64_t, std::shared_ptr<Tracer>> snapshots;

    // Block that is currently processing.
    //
    vm::BasicBlock* block;

    // Terminate block exploration loop.
    //
    bool terminate;
};

```

`src/il/optimizer.cpp`:

```cpp
#include "optimizer.hpp"
#include "logger.hpp"
#include "utils.hpp"

#include "passes/alias.hpp"
#include "passes/coalescing.hpp"
#include "passes/flags_synthesis.hpp"
#include "passes/deps.hpp"

#include <llvm/IR/Verifier.h>
#include <llvm/Passes/PassBuilder.h>
#include <llvm/Analysis/CycleAnalysis.h>
#include <llvm/Analysis/ScopedNoAliasAA.h>
#include <llvm/Analysis/BasicAliasAnalysis.h>
#include <llvm/Analysis/TypeBasedAliasAnalysis.h>
#include <llvm/Analysis/CFLAndersAliasAnalysis.h>
#include <llvm/Analysis/CFLSteensAliasAnalysis.h>
#include <llvm/Analysis/ScalarEvolutionAliasAnalysis.h>

#include <llvm/Transforms/Utils/Cloning.h>
#include <llvm/Transforms/Scalar/LoopRotation.h>

namespace il
{
void replace_undefined_variable(llvm::Function* fn)
{
    if (auto undef = fn->getParent()->getGlobalVariable("__undef"))
    {
        for (auto user : undef->users())
        {
            if (auto load = llvm::dyn_cast<llvm::LoadInst>(user))
            {
                if (auto parent = load->getParent()->getParent())
                {
                    if (parent == fn)
                    {
                        load->replaceAllUsesWith(llvm::UndefValue::get(load->getType()));
                    }
                }
            }
        }
    }
}

bool inline_intrinsics(llvm::Function* fn)
{
    std::set<llvm::CallInst*> calls_to_inline;
    for (auto& bb : *fn)
    {
        for (auto& ins : bb)
        {
            if (auto call = llvm::dyn_cast<llvm::CallInst>(&ins))
            {
                auto called_fn = call->getCalledFunction();
                if (called_fn->hasFnAttribute(llvm::Attribute::AlwaysInline) && !called_fn->isDeclaration())
                {
                    calls_to_inline.insert(call);
                }
            }
        }
    }
    for (auto call : calls_to_inline)
    {
        llvm::InlineFunctionInfo ifi;
        llvm::InlineFunction(*call, ifi);
    }
    return !calls_to_inline.empty();
}

void strip_names(llvm::Function* fn)
{
    for (auto& bb : *fn)
    {
        bb.setName("");
        for (auto& ins : bb)
        {
            if (ins.hasName()) ins.setName("");
        }
    }
}

template<typename M, typename A, typename O>
void exhaust_optimizations(M& manager,  A& analysis, O& object, uint64_t max_tries)
{
    auto inscount{ object.getInstructionCount() };
    auto tries{ 0u };
    while (true)
    {
        manager.run(object, analysis);
        if (inscount > object.getInstructionCount())
        {
            inscount = object.getInstructionCount();
            tries    = 0;
        }
        else if (tries++ > max_tries)
        {
            break;
        }
    }
}

void optimize_function(llvm::Function* fn, const opt_guide& guide)
{
    llvm::AAManager aam;
    llvm::PassBuilder pb;
    llvm::LoopPassManager lpm;
    llvm::LoopAnalysisManager lam;
    llvm::CGSCCAnalysisManager cam;
    llvm::ModuleAnalysisManager mam;
    llvm::FunctionAnalysisManager fam;

    auto ofpm = pb.buildFunctionSimplificationPipeline(guide.level, llvm::ThinOrFullLTOPhase::None);
    auto ompm = pb.buildModuleOptimizationPipeline(guide.level, llvm::ThinOrFullLTOPhase::None);

    ofpm.addPass(llvm::createFunctionToLoopPassAdaptor(llvm::LoopRotatePass(), true, true, true));
    // ofpm.addPass(MemoryCoalescingPass());
    ofpm.addPass(llvm::VerifierPass());

    while (inline_intrinsics(fn))
        ;

    if (guide.alias_analysis)
    {
        aam.registerFunctionAnalysis<SegmentsAA>();
        aam.registerFunctionAnalysis<llvm::BasicAA>();
        aam.registerFunctionAnalysis<llvm::ScopedNoAliasAA>();
        aam.registerFunctionAnalysis<llvm::TypeBasedAA>();
        aam.registerFunctionAnalysis<llvm::CFLAndersAA>();
        aam.registerFunctionAnalysis<llvm::CFLSteensAA>();
        fam.registerPass([]    { return SegmentsAA();   });
        fam.registerPass([aam] { return std::move(aam); });
    }

    pb.registerLoopAnalyses(lam);
    pb.registerCGSCCAnalyses(cam);
    pb.registerModuleAnalyses(mam);
    pb.registerFunctionAnalyses(fam);
    pb.crossRegisterProxies(lam, fam, cam, mam);

    exhaust_optimizations(ofpm, fam, *fn, 2);

    if (guide.remove_undef)
    {
        replace_undefined_variable(fn);
    }

    exhaust_optimizations(ofpm, fam, *fn, 5);

    if (guide.apply_dse)
    {
        ofpm.addPass(MemoryDependenciesPass());
        ofpm.addPass(FlagsSynthesisPass());
    }

    ofpm.run(*fn, fam);

    if (guide.strip_names)
        strip_names(fn);

    if (guide.run_on_module)
    {
        exhaust_optimizations(ompm, mam, *fn->getParent(), 5);
    }

    cam.clear();
    lam.clear();
    fam.clear();
    mam.clear();
}

void optimize_block_function(llvm::Function* fn)
{
    optimize_function(fn, {
        .strip_names = true,
        .level       = llvm::OptimizationLevel::O3
    });
}

void optimize_virtual_function(llvm::Function* fn)
{
    optimize_function(fn, {
        .remove_undef   = true,
        .run_on_module  = true,
        .strip_names    = true,
        .alias_analysis = true,
        .apply_dse      = true,
        .level          = llvm::OptimizationLevel::O3
    });
}
}

```

`src/il/optimizer.hpp`:

```hpp
#pragma once

namespace llvm
{
class Function;
class Module;
};

#include <llvm/Passes/OptimizationLevel.h>

namespace il
{
struct opt_guide
{
    bool remove_undef;
    bool run_on_module;
    bool strip_names;
    bool alias_analysis;
    bool apply_dse;
    llvm::OptimizationLevel level;
};

void optimize_function(llvm::Function* fn, const opt_guide& guide);
void optimize_block_function(llvm::Function* fn);
void optimize_virtual_function(llvm::Function* fn);
};

```

`src/il/passes/alias.cpp`:

```cpp
#include "alias.hpp"
#include "logger.hpp"

#include <llvm/IR/Instruction.h>
#include <llvm/IR/Instructions.h>
#include <llvm/IR/PatternMatch.h>

using namespace llvm::PatternMatch;

#include <set>
#include <stack>

enum class pointer_t
{
    unknown,
    memory_array,
    memory_slot,
    stack_array,
    stack_slot
};

bool is_stack_slot(const llvm::Value* ptr)
{
    llvm::Value* value          = nullptr;
    llvm::ConstantInt* constant = nullptr;

    auto pattern0 = m_Add(m_Load(m_Value(value)), m_ConstantInt(constant));
    auto pattern1 = m_Add(m_Value(value), m_ConstantInt(constant));
    auto pattern2 = m_Load(m_Value(value));

    if (pattern0.match(ptr) ||
        pattern1.match(ptr) ||
        pattern2.match(ptr))
    {
        if (auto arg = llvm::dyn_cast<llvm::Argument>(value))
        {
            if (arg->getName().endswith("sp"))
            {
                return true;
            }
        }
    }
    return false;
}

pointer_t get_pointer_type(const llvm::Value* ptr)
{
    if (auto gep = llvm::dyn_cast<llvm::GEPOperator>(ptr))
    {
        if (auto gv = llvm::dyn_cast<llvm::GlobalVariable>(gep->getPointerOperand()))
        {
            if (gv->getName() == "RAM" && gep->getNumIndices() == 2)
            {
                if (is_stack_slot(gep->getOperand(2)))
                    return pointer_t::stack_slot;

                std::set<const llvm::Value*>    known;
                std::stack<const llvm::Value*>  worklist{ { gep->getOperand(2) } };
                std::vector<const llvm::Value*> bases;

                while (!worklist.empty())
                {
                    auto value = worklist.top(); worklist.pop();

                    if (known.contains(value))
                        continue;
                    known.insert(value);

                    if (auto load = llvm::dyn_cast<llvm::LoadInst>(value))
                    {
                        bases.push_back(load->getPointerOperand());
                        continue;
                    }
                    else if (auto arg = llvm::dyn_cast<llvm::Argument>(value))
                    {
                        bases.push_back(arg);
                        continue;
                    }
                    else if (auto call = llvm::dyn_cast<llvm::CallInst>(value))
                    {
                        auto name = call->getCalledFunction()->getName();
                        if (!name.startswith("llvm.ctpop") && !name.startswith("llvm.fshr") && !name.startswith("llvm.fshl"))
                        {
                            logger::warn("unknown pointer call instruction:");
                            value->dump();
                            return pointer_t::unknown;
                        }
                    }
                    else if (!llvm::isa<llvm::BinaryOperator>(value)
                        && !llvm::isa<llvm::SelectInst>(value)
                        && !llvm::isa<llvm::TruncInst>(value)
                        && !llvm::isa<llvm::ZExtInst>(value)
                        && !llvm::isa<llvm::SExtInst>(value)
                        && !llvm::isa<llvm::ICmpInst>(value)
                        && !llvm::isa<llvm::PHINode>(value))
                    {
                        logger::warn("unknown instruction:");
                        value->dump();
                        return pointer_t::unknown;
                    }

                    if (auto insn = llvm::dyn_cast<llvm::Instruction>(value))
                    {
                        for (const auto& use : insn->operands())
                        {
                            if (llvm::isa<llvm::Instruction>(use.get()) || llvm::isa<llvm::Argument>(use.get()))
                                worklist.push(use.get());
                        }
                    }
                }

                if (bases.size() == 2)
                    return pointer_t::memory_array;
            }
        }
    }
    return pointer_t::unknown;
}



bool SegmentsAAResult::invalidate(llvm::Function& f, const llvm::PreservedAnalyses& pa, llvm::FunctionAnalysisManager::Invalidator& inv)
{
    return false;
}

/*
Differentiate between:
- memory array
- memory slot
- stack array
- stack slot
*/
llvm::AliasResult SegmentsAAResult::alias(const llvm::MemoryLocation& loc_a, const llvm::MemoryLocation& loc_b, llvm::AAQueryInfo& info)
{
    auto a_ty = get_pointer_type(loc_a.Ptr);
    auto b_ty = get_pointer_type(loc_b.Ptr);

    if (a_ty != pointer_t::unknown && b_ty != pointer_t::unknown && a_ty != b_ty)
    {
        return llvm::AliasResult::NoAlias;
    }
    return AAResultBase::alias(loc_a, loc_b, info);
}

llvm::AnalysisKey SegmentsAA::Key;

SegmentsAAResult SegmentsAA::run(llvm::Function& f, llvm::FunctionAnalysisManager& fam)
{
    return SegmentsAAResult();
}

```

`src/il/passes/alias.hpp`:

```hpp
#pragma once

#include <llvm/IR/PassManager.h>
#include <llvm/Analysis/AliasAnalysis.h>

// Based on https://secret.club/2021/09/08/vmprotect-llvm-lifting-3.html#segmentsaa
//
struct SegmentsAAResult : public llvm::AAResultBase<SegmentsAAResult>
{
    bool invalidate(llvm::Function& f, const llvm::PreservedAnalyses& pa, llvm::FunctionAnalysisManager::Invalidator& inv);
    llvm::AliasResult alias(const llvm::MemoryLocation& loc_a, const llvm::MemoryLocation& loc_b, llvm::AAQueryInfo& info);

private:
    friend llvm::AAResultBase<SegmentsAAResult>;
};

struct SegmentsAA final : public llvm::AnalysisInfoMixin<SegmentsAA>
{
    using Result = SegmentsAAResult;

    SegmentsAAResult run(llvm::Function& f, llvm::FunctionAnalysisManager& fam);

    static llvm::AnalysisKey Key;

private:
    friend llvm::AnalysisInfoMixin<SegmentsAA>;
};

```

`src/il/passes/coalescing.cpp`:

```cpp
#include "coalescing.hpp"
#include "logger.hpp"

#include <map>
#include <vector>

#include <llvm/IR/Function.h>
#include <llvm/IR/IRBuilder.h>
#include <llvm/IR/Constants.h>
#include <llvm/IR/PassManager.h>
#include <llvm/Analysis/ScalarEvolutionExpressions.h>

llvm::Type* get_integer_type(int size, llvm::LLVMContext& context)
{
    switch (size)
    {
    case 1:
        return llvm::Type::getInt8Ty(context);
    case 2:
        return llvm::Type::getInt16Ty(context);
    case 4:
        return llvm::Type::getInt32Ty(context);
    case 8:
        return llvm::Type::getInt64Ty(context);
    default:
        logger::error("got unsupported size: {}", size);
    }
}

MemoryAccess::MemoryAccess(llvm::MemoryLocation location, const llvm::MemoryAccess* access, const llvm::SCEV* scev)
    : access_   { access }
    , scev_     { scev }
    , location_ { std::move(location) }
    , size_     { location_.Size.getValue() }
    , offset_   {}
    , supported_{}
{
    auto add = llvm::dyn_cast_or_null<llvm::SCEVAddExpr>(scev_);
    if (add && add->getNumOperands() == 3)
    {
        auto op0 = add->getOperand(0);
        auto op1 = add->getOperand(1);
        auto op2 = add->getOperand(2);
        // (-60 + %1 + @RAM).
        //
        if (op0->getSCEVType() == llvm::scConstant && op1->getSCEVType() == llvm::scUnknown && op2->getSCEVType() == llvm::scUnknown)
        {
            supported_ = true;
            offset_    = (int64_t)llvm::dyn_cast<llvm::SCEVConstant>(op0)->getValue()->getValue().getLimitedValue();
        }
    }
}

const llvm::MemoryAccess* MemoryAccess::access() const noexcept
{
    return access_;
}

const llvm::SCEV* MemoryAccess::scalar() const noexcept
{
    return scev_;
}

const llvm::MemoryLocation& MemoryAccess::location() const noexcept
{
    return location_;
}

uint64_t MemoryAccess::size() const noexcept
{
    return size_;
}

int64_t MemoryAccess::offset() const noexcept
{
    return offset_;
}

bool MemoryAccess::supported() const noexcept
{
    return supported_;
}

llvm::PreservedAnalyses MemoryCoalescingPass::run(llvm::Function& fn, llvm::FunctionAnalysisManager& am)
{
    auto& aam  = am.getResult<llvm::AAManager>(fn);
    auto& msaa = am.getResult<llvm::MemorySSAAnalysis>(fn).getMSSA();
    auto& se   = am.getResult<llvm::ScalarEvolutionAnalysis>(fn);
    auto& dt   = msaa.getDomTree();
    auto pdt   = llvm::PostDominatorTree(fn);

    bool modified = false;

    std::vector<llvm::StoreInst*> garbage;

    for (auto& block : fn)
    {
        // Skip if the block does not have memory accesses.
        //
        if (msaa.getBlockAccesses(&block) == nullptr)
            continue;
        // Find two sequential stores with the same size:
        // ; 2 = MemoryDef(1)
        //   store i16 0, ptr %12, align 1, !noalias !38
        //   %13 = trunc i64 %2 to i16
        // ; 3 = MemoryDef(2)
        //   store i16 %13, ptr %10, align 1, !noalias !38
        //
        // And replace them with 1 store double the size.
        //
        std::vector<const llvm::MemoryAccess*> accs;
        for (const auto& acc : *msaa.getBlockAccesses(&block))
        {
            accs.push_back(&acc);
        }

        for (int i = 0; i < accs.size() - 1; i++)
        {
            auto def0 = llvm::dyn_cast_or_null<llvm::MemoryDef>(accs[i]);
            auto def1 = llvm::dyn_cast_or_null<llvm::MemoryDef>(accs[i + 1]);

            if (def0 && def1)
            {
                auto insn0 = llvm::dyn_cast_or_null<llvm::StoreInst>(def0->getMemoryInst());
                auto insn1 = llvm::dyn_cast_or_null<llvm::StoreInst>(def1->getMemoryInst());

                auto store0 = MemoryAccess(llvm::MemoryLocation::get(insn0), msaa.getMemoryAccess(insn0), se.getSCEV(insn0->getPointerOperand()));
                auto store1 = MemoryAccess(llvm::MemoryLocation::get(insn1), msaa.getMemoryAccess(insn1), se.getSCEV(insn1->getPointerOperand()));

                if (store0.supported() && store1.supported() && store0.size() == store1.size() && store0.size() < 8)
                {
                    if (store1.offset() + store1.size() == store0.offset())
                    {
                        logger::debug("Found two sequential stores {} {}:", store0.offset(), store1.offset());
                        insn0->dump();
                        insn1->dump();
                        llvm::IRBuilder<> ir(insn1->getNextNode());
                        auto op0_zext  = ir.CreateZExt(insn0->getValueOperand(), ir.getIntNTy(store0.size() * 8 * 2));
                        auto op1_zext  = ir.CreateZExt(insn1->getValueOperand(), ir.getIntNTy(store1.size() * 8 * 2));
                        auto op0_shl   = ir.CreateShl(op0_zext, store0.size() * 8, "", true, true);
                        auto fin_or    = ir.CreateOr(op0_shl, op1_zext);
                        auto fin_store = ir.CreateStore(fin_or, insn1->getPointerOperand());
                        garbage.push_back(insn0);
                        garbage.push_back(insn1);

                        i++;
                        modified = true;
                    }
                }
            }
        }
    }
    for (auto& store : garbage)
    {
        store->eraseFromParent();
    }
    return modified ? llvm::PreservedAnalyses::none() : llvm::PreservedAnalyses::all();
}

```

`src/il/passes/coalescing.hpp`:

```hpp
#pragma once

#include <llvm/IR/PassManager.h>
#include <llvm/IR/Instructions.h>
#include <llvm/Analysis/MemorySSA.h>
#include <llvm/Analysis/AliasAnalysis.h>
#include <llvm/Analysis/PostDominators.h>
#include <llvm/Analysis/ScalarEvolution.h>
#include <llvm/Analysis/ScalarEvolutionAliasAnalysis.h>

struct MemoryAccess
{
    MemoryAccess(llvm::MemoryLocation location, const llvm::MemoryAccess* access, const llvm::SCEV* scev);

    const llvm::MemoryAccess*   access()      const noexcept;
    const llvm::SCEV*           scalar()      const noexcept;
    const llvm::MemoryLocation& location()    const noexcept;
    uint64_t                    size()        const noexcept;
    int64_t                     offset()      const noexcept;
    bool                        supported()   const noexcept;

private:
    const llvm::MemoryAccess*  access_;
    const llvm::SCEV*          scev_;
    const llvm::MemoryLocation location_;
    // Size of the memory load.
    //
    uint64_t size_;
    // Offset within RAM. e.g. in case of SCEV (-60 + %1 + @RAM) offset will be -60.
    //
    int64_t offset_;
    // If load is supported by MemoryCoalescingPass.
    //
    bool supported_;
};

struct MemoryCoalescingPass final : public llvm::PassInfoMixin<MemoryCoalescingPass>
{
    llvm::PreservedAnalyses run(llvm::Function &fn, llvm::FunctionAnalysisManager &am);
};

```

`src/il/passes/deps.cpp`:

```cpp
#include "deps.hpp"
#include "logger.hpp"
#include <llvm/Analysis/AliasAnalysis.h>
#include <llvm/Analysis/PostDominators.h>
#include <llvm/Analysis/ScalarEvolution.h>
#include <llvm/Analysis/MemoryDependenceAnalysis.h>
#include <llvm/Analysis/ScalarEvolutionAliasAnalysis.h>

llvm::PreservedAnalyses MemoryDependenciesPass::run(llvm::Function& fn, llvm::FunctionAnalysisManager& am)
{
    auto& mda = am.getResult<llvm::MemoryDependenceAnalysis>(fn);
    for (auto& bb : fn)
    {
        for (auto& insn : bb)
        {
            if (auto store = llvm::dyn_cast<llvm::StoreInst>(&insn))
            {
                auto dep  = mda.getDependency(store);
                auto insn = dep.getInst();
                if (insn)
                {
                    logger::debug("memory dependence:");
                    store->dump();
                    insn->dump();
                }
                else
                {
                    logger::debug("no memory dependence:");
                    store->dump();
                }
            }
        }
    }
    return llvm::PreservedAnalyses::all();
}

```

`src/il/passes/deps.hpp`:

```hpp
#pragma once

#include <llvm/IR/PassManager.h>

struct MemoryDependenciesPass final : public llvm::PassInfoMixin<MemoryDependenciesPass>
{
    llvm::PreservedAnalyses run(llvm::Function& fn, llvm::FunctionAnalysisManager& fam);
};

```

`src/il/passes/flags_synthesis.cpp`:

```cpp
#include "flags_synthesis.hpp"
#include "il/solver.hpp"
#include "logger.hpp"
#include "utils.hpp"

#include <llvm/IR/IRBuilder.h>
#include <llvm/IR/Dominators.h>
#include <llvm/IR/Instruction.h>
#include <llvm/IR/InstIterator.h>
#include <llvm/IR/Instructions.h>

#include <triton/llvmToTriton.hpp>

#include <optional>
#include <fstream>
#include <stack>

struct InstructionSlice
{
    static InstructionSlice get(llvm::Instruction* value)
    {
        InstructionSlice slice;

        std::stack<llvm::Instruction*> worklist{ { value } };
        std::set<llvm::Instruction*> known;

        while (!worklist.empty())
        {
            auto insn = worklist.top(); worklist.pop();
            // Skip if known.
            //
            if (known.contains(insn))
                continue;
            // Save.
            //
            known.insert(insn);
            slice.stream.push_back(insn);
            // Terminate if load/phi.
            //
            if (llvm::isa<llvm::LoadInst>(insn) || llvm::isa<llvm::PHINode>(insn))
            {
                slice.operands.push_back(insn);
                continue;
            }
            // Iterate use chain.
            //
            for (const auto& op : insn->operands())
            {
                if (auto op_insn = llvm::dyn_cast<llvm::Instruction>(op.get()))
                {
                    worklist.push(op_insn);
                }
            }
        }
        // Sort instructions by dominance.
        //
        llvm::DominatorTree dt(*value->getFunction());

        std::sort(slice.stream.begin(), slice.stream.end(), [&dt](const auto& a, const auto& b)
        {
            return dt.dominates(a, b);
        });
        return slice;
    }

    std::vector<llvm::Instruction*> stream;
    std::vector<llvm::Instruction*> operands;
};

#define GET_OR_CREATE_FUNCTION(name)                            \
    if (auto fn = module->getFunction(name))                    \
        return fn;                                              \
    auto fn = llvm::Function::Create(                           \
        llvm::FunctionType::get(i1, { ptr, ptr }, false),       \
        llvm::GlobalVariable::LinkageTypes::InternalLinkage,    \
        name,                                                   \
        module.get()                                            \
    );                                                          \
    auto bb  = llvm::BasicBlock::Create(*context, "body", fn);  \
    llvm::IRBuilder<> ir(bb);                                   \
    auto op0 = ir.CreateLoad(i64, fn->getOperand(0));           \
    auto op1 = ir.CreateLoad(i64, fn->getOperand(1));           \



llvm::Function* FlagsSynthesisPass::get_or_create_jo()
{
    GET_OR_CREATE_FUNCTION("jo");
    ir.CreateRet(
        ir.CreateExtractValue(
            ir.CreateCall(
                llvm::Intrinsic::getDeclaration(module.get(), llvm::Intrinsic::sadd_with_overflow, i64),
                { op1, ir.CreateXor(op0, -1) }
            ),
            { 1 }
        )
    );
    return fn;
}

llvm::Function* FlagsSynthesisPass::get_or_create_js()
{
    GET_OR_CREATE_FUNCTION("js");
    ir.CreateRet(
        ir.CreateICmpSGT(
            ir.CreateSub(
                op0, op1
            ),
            llvm::ConstantInt::get(i64, -1)
        )
    );
    return fn;
}

llvm::Function* FlagsSynthesisPass::get_or_create_jns()
{
    GET_OR_CREATE_FUNCTION("jns");
    ir.CreateRet(
        ir.CreateICmpSLT(
            ir.CreateSub(
                op0, op1
            ),
            llvm::ConstantInt::get(i64, 0)
        )
    );
    return fn;
}

llvm::Function* FlagsSynthesisPass::get_or_create_je()
{
    GET_OR_CREATE_FUNCTION("je");
    ir.CreateRet(
        ir.CreateICmpEQ(
            op0, op1
        )
    );
    return fn;
}

llvm::Function* FlagsSynthesisPass::get_or_create_jne()
{
    GET_OR_CREATE_FUNCTION("jne");
    ir.CreateRet(
        ir.CreateICmpEQ(
            op1, op0
        )
    );
    return fn;
}

llvm::Function* FlagsSynthesisPass::get_or_create_jb()
{
    GET_OR_CREATE_FUNCTION("jb");
    ir.CreateRet(
        ir.CreateICmpUGT(
            op1, op0
        )
    );
    return fn;
}

llvm::Function* FlagsSynthesisPass::get_or_create_ja()
{
    GET_OR_CREATE_FUNCTION("ja");
    ir.CreateRet(
        ir.CreateICmpULT(op1, op0)
    );
    return fn;
}

llvm::Function* FlagsSynthesisPass::get_or_create_jl()
{
    GET_OR_CREATE_FUNCTION("jl");
    ir.CreateRet(
        ir.CreateICmpSGT(
            op1, op0
        )
    );
    return fn;
}

llvm::Function* FlagsSynthesisPass::get_or_create_jge()
{
    GET_OR_CREATE_FUNCTION("jge");
    ir.CreateRet(
        ir.CreateICmpSLT(
            op0, op1
        )
    );
    return fn;
}

llvm::Function* FlagsSynthesisPass::get_or_create_jle()
{
    GET_OR_CREATE_FUNCTION("jle");
    ir.CreateRet(
        ir.CreateICmpSLT(
            op1, op0
        )
    );
    return fn;
}

llvm::Function* FlagsSynthesisPass::get_or_create_jg()
{
    GET_OR_CREATE_FUNCTION("jg");
    ir.CreateRet(
        ir.CreateICmpSGT(
            op0, op1
        )
    );
    return fn;
}

// %1 = load i64, i64* %rax, align 8
// %2 = load i64, i64* %rbx, align 8
// %5 = xor i64 %1, -1
// %6 = add i64 %2, %5
// %7 = trunc i64 %6 to i32
// %8 = and i32 %7, 255
// %9 = call i32 @llvm.ctpop.i32(i32 %8) #15
// %10 = and i32 %9, 1
// %11 = icmp eq i32 %10, 0
//
llvm::Function* FlagsSynthesisPass::get_or_create_jp()
{
    GET_OR_CREATE_FUNCTION("jp");
    ir.CreateRet(
        ir.CreateICmpEQ(
            ir.CreateAnd(
                ir.CreateCall(
                    llvm::Intrinsic::getDeclaration(module.get(), llvm::Intrinsic::ctpop, i32),
                    {
                        ir.CreateAnd(
                            ir.CreateTrunc(
                                ir.CreateAdd(
                                    op1,
                                    ir.CreateXor(
                                        op0, llvm::ConstantInt::get(i64, -1)
                                    )
                                ),
                                i32
                            ),
                            llvm::ConstantInt::get(i32, 255)
                        )
                    }
                ),
                llvm::ConstantInt::get(i32, 1)
            ),
            llvm::ConstantInt::get(i32, 0)
        )
    );
    return fn;
}

FlagsSynthesisPass::FlagsSynthesisPass()
    : context{ new llvm::LLVMContext }
    , module { new llvm::Module("flags", *context) }
    , i1 { llvm::IntegerType::getInt1Ty (*context) }
    , i32{ llvm::IntegerType::getInt32Ty(*context) }
    , i64{ llvm::IntegerType::getInt64Ty(*context) }
    , ptr{ llvm::PointerType::get(*context, 0)     }
{
}

llvm::PreservedAnalyses FlagsSynthesisPass::run(llvm::Function& fn, llvm::FunctionAnalysisManager& am)
{
    auto module = fn.getParent();
    auto layout = module->getDataLayout();

    for (auto& insn : llvm::instructions(fn))
    {
        if (auto br = llvm::dyn_cast<llvm::BranchInst>(&insn))
        {
            if (br->isConditional())
            {
                if (auto condition = llvm::dyn_cast<llvm::Instruction>(br->getOperand(0)))
                {
                    auto slice = InstructionSlice::get(condition);

                    // logger::info("slicing..");
                    // for (const auto& insn : slice.stream)
                    // {
                    //     insn->dump();
                    // }
                }
            }
        }
    }
    return llvm::PreservedAnalyses::none();
}

```

`src/il/passes/flags_synthesis.hpp`:

```hpp
#pragma once

#include <llvm/IR/PassManager.h>

// Based on https://godbolt.org/z/aYdc3xhn9.
//
struct FlagsSynthesisPass final : public llvm::PassInfoMixin<FlagsSynthesisPass>
{
    FlagsSynthesisPass();
    llvm::PreservedAnalyses run(llvm::Function& fn, llvm::FunctionAnalysisManager& fam);

private:
    llvm::Function* get_or_create_jo();
    /* llvm::Function* get_or_create_jno(); */
    llvm::Function* get_or_create_js();
    llvm::Function* get_or_create_jns();
    llvm::Function* get_or_create_je();
    llvm::Function* get_or_create_jne();
    llvm::Function* get_or_create_jb();
    /* llvm::Function* get_or_create_jnb(); */
    /* llvm::Function* get_or_create_jna(); */
    llvm::Function* get_or_create_ja();
    llvm::Function* get_or_create_jl();
    llvm::Function* get_or_create_jge();
    llvm::Function* get_or_create_jle();
    llvm::Function* get_or_create_jg();
    llvm::Function* get_or_create_jp();
    /* llvm::Function* get_or_create_jnp(); */

    std::unique_ptr<llvm::LLVMContext> context;
    std::unique_ptr<llvm::Module> module;

    llvm::Type* i1;
    llvm::Type* i32;
    llvm::Type* i64;
    llvm::Type* ptr;
};

```

`src/il/solver.cpp`:

```cpp
#include "asserts.hpp"
#include "solver.hpp"
#include "logger.hpp"
#include "utils.hpp"

#include <fstream>

#include <llvm/IR/Module.h>
#include <llvm/Support/CommandLine.h>
#include <llvm/IR/InstrTypes.h>
#include <llvm/IR/Instruction.h>

#include <triton/llvmToTriton.hpp>

llvm::cl::opt<bool> save_branch_ast("solver-save-ast",
    llvm::cl::desc("Save branch ast into dot file on every branch."),
    llvm::cl::value_desc("flag"),
    llvm::cl::init(false),
    llvm::cl::Optional);

llvm::cl::opt<bool> print_branch_ast("solver-print-ast",
    llvm::cl::desc("Print branch ast on every branch."),
    llvm::cl::value_desc("flag"),
    llvm::cl::init(false),
    llvm::cl::Optional);

namespace il
{
std::vector<uint64_t> get_possible_targets(llvm::Value* ret)
{
    if (ret == nullptr)
    {
        logger::error("get_possible_targets argument got null argument.");
    }

    std::vector<uint64_t> targets;
    // Does not matter which arch we use.
    //
    triton::Context api(triton::arch::ARCH_X86_64);
    api.setAstRepresentationMode(triton::ast::representations::SMT_REPRESENTATION);

    if (auto inst = llvm::dyn_cast<llvm::Instruction>(ret))
    {
        if (inst->getOpcode() == llvm::Instruction::Or)
        {
            logger::warn("replacing or with add.");
            llvm::IRBuilder<> ir(inst);
            ret = ir.CreateAdd(inst->getOperand(0), inst->getOperand(1));
            inst->replaceAllUsesWith(ret);
        }
    }
    triton::ast::LLVMToTriton lifter(api);
    // Lift llvm into triton ast.
    //
    auto node = lifter.convert(ret);

    if (save_branch_ast)
    {
        static int solver_temp_names;
        std::fstream fd(fmt::format("branch-ast-{}.dot", solver_temp_names++), std::ios_base::out);
        if (fd.good())
            api.liftToDot(fd, node);
    }
    if (print_branch_ast)
    {
        logger::info("branch ast: {}", triton::ast::unroll(node));
    }
    // If constant.
    //
    if (!node->isSymbolized())
    {
        return { static_cast<uint64_t>(node->evaluate()) };
    }
    auto ast         = api.getAstContext();
    auto zero        = ast->bv(0, node->getBitvectorSize());
    auto constraints = ast->distinct(node, zero);

    while (true)
    {
        // Failsafe.
        //
        if (targets.size() > 2)
            return {};

        auto model = api.getModel(constraints);

        if (model.empty())
            break;
        for (auto& [id, sym] : model)
            api.setConcreteVariableValue(api.getSymbolicVariable(id), sym.getValue());

        auto target = static_cast<uint64_t>(node->evaluate());
        targets.push_back(target);
        // Update constraints.
        //
        constraints = ast->land(constraints, ast->distinct(node, ast->bv(target, node->getBitvectorSize())));
    }
    return targets;
}
};

```

`src/il/solver.hpp`:

```hpp
#pragma once
#include <vector>
#include <cstdint>
#include <unordered_map>

#include <triton/context.hpp>
#include <triton/llvmToTriton.hpp>

#include <llvm/IR/IRBuilder.h>
#include <llvm/IR/LLVMContext.h>
#include <llvm/IR/Module.h>

namespace llvm
{
class Value;
};

namespace il
{
std::vector<uint64_t> get_possible_targets(llvm::Value* ret);
};

```

`src/lifter.cpp`:

```cpp
#include "lifter.hpp"
#include "logger.hpp"
#include "utils.hpp"

#include "vm/instruction.hpp"
#include "vm/routine.hpp"

#include <llvm/IR/IRBuilder.h>
#include <llvm/IRReader/IRReader.h>
#include <llvm/Support/SourceMgr.h>
#include <llvm/Support/CommandLine.h>
#include <llvm/Transforms/Utils/Cloning.h>
#include <map>

llvm::cl::opt<std::string> intrinsics("i",
    llvm::cl::desc("Path to vmprotect intrinsics file"),
    llvm::cl::value_desc("intrinsics"),
    llvm::cl::Required);

llvm::Value* ReturnArguments::return_address() const noexcept
{
    return ret;
}

llvm::Value* ReturnArguments::program_counter() const noexcept
{
    return rip;
}

Lifter::Lifter() : ir(context)
{
    llvm::SMDiagnostic err;
    auto parsed = llvm::parseIRFile(intrinsics, err, context);
    if (parsed == nullptr)
    {
        logger::error("Lifter::Lifter: Failed to parse intrinsics file");
    }
    module = std::move(parsed);
    // Extract helper functions.
    //
    helper_lifted_fn      = module->getFunction("VirtualFunction");
    helper_empty_block_fn = module->getFunction("VirtualStubEmpty");
    helper_block_fn       = module->getFunction("VirtualStub");
    helper_keep_fn        = module->getFunction("KeepReturn");
    helper_slice_fn       = module->getFunction("SlicePC");
    helper_undef          = module->getGlobalVariable("__undef");
    if (helper_lifted_fn == nullptr)
        logger::error("Failed to find VirtualFunction function");
    if (helper_empty_block_fn == nullptr)
        logger::error("Failed to find VirtualStubEmpty function");
    if (helper_block_fn == nullptr)
        logger::error("Failed to find VirtualStub function");
    if (helper_keep_fn == nullptr)
        logger::error("Failed to find KeepPC function");
    if (helper_slice_fn == nullptr)
        logger::error("Failed to find SlicePC function");
    if (helper_undef == nullptr)
        logger::error("Failed to find global undef variable");
    // Collect semantics functions.
    //
    for (const auto& glob : module->globals())
    {
        const auto& name = glob.getName();
        if (name.startswith("SEM_") && glob.isConstant() && glob.getType()->isPointerTy())
        {
            const auto initializer = glob.getInitializer();
            auto resolved_fn       = module->getFunction(initializer->getName());
            if (resolved_fn == nullptr)
            {
                logger::error("Lifter::Lifter: Failed to resolve function for global {}", name.str());
            }
            sems.emplace(name.str().substr(4), resolved_fn);
        }
    }
}

llvm::Function* Lifter::lift_basic_block(vm::BasicBlock* vblock)
{
    // Copy empty block function.
    //
    function = clone(helper_empty_block_fn);
    // Remove bad attributes. Don't know if it does anything but still.
    //
    for (auto& arg : function->args())
    {
        arg.removeAttr(llvm::Attribute::ReadNone);
        arg.removeAttr(llvm::Attribute::ReadOnly);
    }
    function->removeFnAttr(llvm::Attribute::ReadNone);
    // Remove entry basic block and insert new one.
    //
    function->getEntryBlock().eraseFromParent();

    ir.SetInsertPoint(llvm::BasicBlock::Create(context, "lifted_bb", function));
    // Lift instruction stream.
    //
    for (const auto& insn : *vblock)
    {
        std::visit(*this, insn);
    }
    // Return VIP.
    //
    ir.CreateRet(ir.CreateLoad(function->getReturnType(), vip()));
    return function;
}

llvm::Function* Lifter::build_function(const vm::Routine* rtn, uint64_t target_block)
{
    function = clone(helper_empty_block_fn);
    function->getEntryBlock().eraseFromParent();
    auto block = llvm::BasicBlock::Create(context, "entry", function);

    std::vector<llvm::Value*> args;
    for (auto& arg : function->args())
        args.push_back(&arg);
    // Create empty basic blocks for each basic block in the routine.
    //
    std::map<uint64_t, llvm::BasicBlock*> blocks;
    for (const auto [vip, bb] : *rtn)
    {
        blocks.emplace(vip, llvm::BasicBlock::Create(context, fmt::format("bb_0x{:x}", vip), function));
    }
    // Link together llvm basic blocks based on edges in routine and populate with calls to lifted functions.
    //
    for (auto [vip, bb] : blocks)
    {
        ir.SetInsertPoint(bb);
        auto vblock = rtn->blocks.at(vip);
        if (vblock->lifted != nullptr)
        {
            auto pc = ir.CreateCall(vblock->lifted, args);
            // Check if we are building partial function and if so, make a call to KeepPC.
            //
            if (vblock->vip() == target_block && target_block != vm::invalid_vip)
            {
                // Load rsp value.
                //
                auto ret = create_memory_read_64(ir.CreateLoad(ir.getInt64Ty(), vsp()));

                pc = ir.CreateCall(helper_keep_fn, { pc, ret });
            }
            // Link successors with the current block.
            //
            switch (vblock->next.size())
            {
                case 0:
                {
                    ir.CreateRet(pc);
                    break;
                }
                case 1:
                {
                    auto dst_vip = vblock->next.at(0)->vip();
                    auto dst_blk = blocks.at(dst_vip);
                    if (vblock->vip() == target_block && target_block != vm::invalid_vip)
                    {
                        // Create dummy basic block.
                        //
                        auto dummy_bb = llvm::BasicBlock::Create(context, fmt::format("bb_dummy_0x{:x}", vblock->vip()), function);
                        llvm::ReturnInst::Create(context, pc, dummy_bb);
                        auto cmp = ir.CreateICmpEQ(pc, ir.getInt64(dst_vip));
                        ir.CreateCondBr(cmp, dst_blk, dummy_bb);
                    }
                    else
                    {
                        ir.CreateBr(dst_blk);
                    }
                    break;
                }
                case 2:
                {
                    auto dst_blk_1 = blocks.at(vblock->next.at(0)->vip());
                    auto dst_blk_2 = blocks.at(vblock->next.at(1)->vip());
                    auto cmp       = ir.CreateICmpEQ(pc, ir.getInt64(vblock->next.at(0)->vip()));
                    ir.CreateCondBr(cmp, dst_blk_1, dst_blk_2);
                    break;
                }
                default:
                    llvm_unreachable("Switch statement is currently not supported.");
            }
        }
        else
        {
            ir.CreateRet(ir.getInt64(0xdeadbeef));
        }
    }
    llvm::BranchInst::Create(blocks.at(rtn->entry->vip()), block);
    // If its a partial function, we want to slice PC, otherwise build "final function".
    //
    if (target_block != vm::invalid_vip)
        return make_slice(function);
    return make_final(function, rtn->entry->vip());
}

ReturnArguments Lifter::get_return_args(llvm::Function* fn) const
{
    for (auto& block : *fn)
    {
        for (auto& ins : block)
        {
            if (auto call = llvm::dyn_cast<llvm::CallInst>(&ins))
            {
                if (call->getCalledFunction() == helper_keep_fn)
                {
                    return ReturnArguments(call->getOperand(0), call->getOperand(1));
                }
            }
        }
    }
    logger::error("Failed to find call to KeepReturnAddress funtion in {}", fn->getName().str());
}

llvm::Argument* Lifter::arg(llvm::Function* fn, const std::string& name)
{
    for (auto& arg : fn->args())
        if (arg.getName().equals(name))
            return &arg;
    return nullptr;
}

llvm::Argument* Lifter::arg(const std::string& name)
{
    return arg(function, name);
}

llvm::Function* Lifter::clone(llvm::Function* fn)
{
    llvm::ValueToValueMapTy map;
    return llvm::CloneFunction(fn, map);
}

llvm::Function* Lifter::sem(const std::string& name) const
{
    if (sems.find(name) == sems.end())
        logger::error("Failed to find {} semantic", name);
    return sems.at(name);
}

llvm::Argument* Lifter::vip()
{
    return arg("vip");
}

llvm::Argument* Lifter::vsp()
{
    return arg("vsp");
}

llvm::Argument* Lifter::vregs()
{
    return arg("vmregs");
}

llvm::Function* Lifter::make_slice(llvm::Function* fn)
{
    auto slice = clone(helper_slice_fn);
    for (auto& ins : slice->getEntryBlock())
    {
        if (auto call = llvm::dyn_cast<llvm::CallInst>(&ins))
        {
            if (call->getCalledFunction() == helper_block_fn)
            {
                call->setCalledFunction(fn);
                break;
            }
        }
    }
    return slice;
}

llvm::Function* Lifter::make_final(llvm::Function* fn, uint64_t addr)
{
    auto* final = clone(helper_lifted_fn);
    for (auto& ins : final->getEntryBlock())
    {
        if (auto* call = llvm::dyn_cast<llvm::CallInst>(&ins))
        {
            if (call->getCalledFunction() == helper_block_fn)
            {
                call->setCalledFunction(fn);
                break;
            }
        }
    }
    return final;
}

void Lifter::operator()(const vm::Add& insn)
{
    ir.CreateCall(sem(fmt::format("ADD_{}", insn.size())), { vsp() });
}

void Lifter::operator()(const vm::Shl& insn)
{
    ir.CreateCall(sem(fmt::format("SHL_{}", insn.size())), { vsp() });
}

void Lifter::operator()(const vm::Shr& insn)
{
    ir.CreateCall(sem(fmt::format("SHR_{}", insn.size())), { vsp() });
}

void Lifter::operator()(const vm::Ldr& insn)
{
    ir.CreateCall(sem(fmt::format("LOAD_{}", insn.size())), { vsp() });
}

void Lifter::operator()(const vm::Str& insn)
{
    ir.CreateCall(sem(fmt::format("STORE_{}", insn.size())), { vsp() });
}

void Lifter::operator()(const vm::Nor& insn)
{
    ir.CreateCall(sem(fmt::format("NOR_{}", insn.size())), { vsp() });
}

void Lifter::operator()(const vm::Nand& insn)
{
    ir.CreateCall(sem(fmt::format("NAND_{}", insn.size())), { vsp() });
}

void Lifter::operator()(const vm::Shrd& insn)
{
    ir.CreateCall(sem(fmt::format("SHRD_{}", insn.size())), { vsp() });
}

void Lifter::operator()(const vm::Shld& insn)
{
    ir.CreateCall(sem(fmt::format("SHLD_{}", insn.size())), { vsp() });
}

void Lifter::operator()(const vm::Push& insn)
{
    auto size = insn.size();

    if (insn.op().is_immediate())
    {
        ir.CreateCall(sem(fmt::format("PUSH_IMM_{}", size)), { vsp(), ir.getInt(llvm::APInt(size, insn.op().imm().value())) });
    }
    else if (insn.op().is_physical())
    {
        auto reg = arg(insn.op().phy().name());
        auto ldr = ir.CreateLoad(ir.getInt64Ty(), reg);
        ir.CreateCall(sem(fmt::format("PUSH_REG_{}", size)), { vsp(), ldr });
    }
    else if (insn.op().is_virtual())
    {
        auto num = insn.op().vrt().number();
        auto off = insn.op().vrt().offset();
        auto gep = ir.CreateInBoundsGEP(vregs()->getType(), vregs(), { ir.getInt(llvm::APInt(size, num)) });
        auto ldr = ir.CreateLoad(ir.getInt64Ty(), gep);
        ir.CreateCall(sem(fmt::format("PUSH_VREG_{}_{}", size, off)), { vsp(), ldr });
    }
    else if (insn.op().is_vsp())
    {
        ir.CreateCall(sem(fmt::format("PUSH_VSP_{}", size)), { vsp() });
    }
    else
    {
        logger::error("Lifter::operator(): Unsupported Push operand.");
    }
}

void Lifter::operator()(const vm::Pop& insn)
{
    auto size = insn.size();

    if (insn.op().is_physical())
    {
        ir.CreateCall(sem(fmt::format("POP_REG_{}", size)), { vsp(), arg(insn.op().phy().name()) });
    }
    else if (insn.op().is_virtual())
    {
        auto num = insn.op().vrt().number();
        auto off = insn.op().vrt().offset();
        auto gep = ir.CreateInBoundsGEP(vregs()->getType(), vregs(), { ir.getInt(llvm::APInt(size, num)) });
        ir.CreateCall(sem(fmt::format("POP_VREG_{}_{}", size, off)), { vsp(), gep });
    }
    else if (insn.op().is_vsp())
    {
        ir.CreateCall(sem(fmt::format("POP_VSP_{}", size)), { vsp() });
    }
    else
    {
        logger::error("Lifter::operator(): Unsupported Pop operand.");
    }
}

void Lifter::operator()(const vm::Jmp& insn)
{
    ir.CreateCall(sem("JMP"), { vsp(), vip() });
}

void Lifter::operator()(const vm::Ret& insn)
{
    ir.CreateCall(sem("RET"), { vsp(), vip() });
}

void Lifter::operator()(const vm::Jcc& insn)
{
    if (insn.direction() == vm::jcc_e::up)
    {
        ir.CreateCall(sem("JCC_INC"), { vsp(), vip() });
    }
    else
    {
        ir.CreateCall(sem("JCC_DEC"), { vsp(), vip() });
    }
}

void Lifter::operator()(const vm::Exit& insn)
{
    for (const auto& reg : insn.regs())
    {
        (*this)(reg);
    }
}

void Lifter::operator()(const vm::Enter& insn)
{
    for (const auto& reg : insn.regs())
    {
        (*this)(reg);
    }
}

llvm::Value* Lifter::create_memory_read_64(llvm::Value* address)
{
    auto ram = module->getGlobalVariable("RAM");
    auto gep = ir.CreateInBoundsGEP(ram->getValueType(), ram, { ir.getInt64(0), address });
    return ir.CreateLoad(ir.getInt64Ty(), gep);
}

llvm::Value* Lifter::create_memory_write_64(llvm::Value* address, llvm::Value* ptr)
{
    auto ram = module->getGlobalVariable("RAM");
    auto gep = ir.CreateInBoundsGEP(ram->getValueType(), ram, { ir.getInt64(0), address });
    return ir.CreateStore(ir.CreateLoad(ir.getInt64Ty(), ptr), gep);
}

std::vector<llvm::BasicBlock*> Lifter::get_exit_blocks(llvm::Function* fn) const
{
    std::vector<llvm::BasicBlock*> exits;

    for (auto& bb : *fn)
    {
        if (auto ret = llvm::dyn_cast<llvm::ReturnInst>(bb.getTerminator()))
        {
            exits.push_back(&bb);
        }
    }
    return exits;
}

// NOTE: This is just for testing.
//
void Lifter::create_external_call(llvm::Function* fn, const std::string& name)
{
    function   = fn;
    auto exits = get_exit_blocks(fn);
    if (exits.size() != 1)
        logger::error("Invalid number ({}) of exit blocks in a function {}", exits.size(), fn->getName().str());
    auto term = exits.back();
    // Insert call right before the ret instruction.
    //
    ir.SetInsertPoint(term->getTerminator()->getPrevNode());

    auto callee_ty = llvm::FunctionType::get(ir.getInt64Ty(), { ir.getInt64Ty(), /* ir.getInt64Ty(), ir.getInt64Ty(), ir.getInt64Ty() */ }, false);
    auto callee_fn = llvm::Function::Create(callee_ty, llvm::GlobalVariable::LinkageTypes::ExternalLinkage, name, module.get());
    // Mark this function as `ReadNone` meaning it does not read memory. This attribute allows for some optimizations to be applied.
    // ref: http://formalverification.cs.utah.edu/llvm_doxy/2.9/namespacellvm_1_1Attribute.html
    //
    callee_fn->addFnAttr(llvm::Attribute::ReadNone);
    // Pop function call address from the stack and mark the slot with __undef.
    // This will remove useless store in the final function.
    //
    ir.CreateCall(sem("STACK_POP_64"), { vsp() });

    auto call = ir.CreateCall(callee_fn, {
        ir.CreateLoad(ir.getInt64Ty(), arg("rcx")),
        // ir.CreateLoad(ir.getInt64Ty(), arg("rdx")),
        // ir.CreateLoad(ir.getInt64Ty(), arg("r8")),
        // ir.CreateLoad(ir.getInt64Ty(), arg("r9"))
    });
    ir.CreateStore(call, arg("rax"));
}

```

`src/lifter.hpp`:

```hpp
#pragma once
#include "vm/routine.hpp"
#include "logger.hpp"

#include <llvm/IR/Module.h>
#include <llvm/IR/Constants.h>
#include <llvm/IR/IRBuilder.h>
#include <llvm/IR/Instructions.h>

struct ReturnArguments
{
    explicit ReturnArguments(llvm::Value* rip, llvm::Value* ret) : rip{ rip }, ret{ ret } {}

    llvm::Value* return_address()  const noexcept;
    llvm::Value* program_counter() const noexcept;

private:
    llvm::Value* rip;
    llvm::Value* ret;
};

// IR to LLVM Lifter class.
//
struct Lifter
{
    Lifter();

    // Lift `basic_block` into llvm function.
    // All basic_blocks represented as llvm::Function's.
    //
    llvm::Function* lift_basic_block(vm::BasicBlock* block);

    // Build paritual or full control flow graph of a routine.
    //
    llvm::Function* build_function(const vm::Routine* routine, uint64_t target_block = vm::invalid_vip);

    // Get program counter and [rsp] value from KeepReturn function.
    //
    ReturnArguments get_return_args(llvm::Function* function) const;

    void create_external_call(llvm::Function* function, const std::string& name);

    void operator()(const vm::Add&);
    void operator()(const vm::Shl&);
    void operator()(const vm::Shr&);
    void operator()(const vm::Ldr&);
    void operator()(const vm::Str&);
    void operator()(const vm::Nor&);
    void operator()(const vm::Nand&);
    void operator()(const vm::Shrd&);
    void operator()(const vm::Shld&);
    void operator()(const vm::Push&);
    void operator()(const vm::Pop&);
    void operator()(const vm::Jmp&);
    void operator()(const vm::Ret&);
    void operator()(const vm::Jcc&);
    void operator()(const vm::Exit&);
    void operator()(const vm::Enter&);

private:
    llvm::Value* create_memory_read_64(llvm::Value* address);
    llvm::Value* create_memory_write_64(llvm::Value* address, llvm::Value* ptr);

    std::vector<llvm::BasicBlock*> get_exit_blocks(llvm::Function* function) const;

    // Get virtual instruction pointer from function arguments.
    //
    llvm::Argument* vip();

    // Get virtual stack pointer from function arguments.
    //
    llvm::Argument* vsp();

    // Get virtual registers array from function arguments.
    //
    llvm::Argument* vregs();

    // Get llvm function for vmp instruction.
    //
    llvm::Function* sem(const std::string& name) const;

    // Get function argument by name.
    //
    llvm::Argument* arg(llvm::Function* fn, const std::string& name);

    // Get function argument by name.
    //
    llvm::Argument* arg(const std::string& name);

    // Shallow copy function into a new one.
    // The global variables which the function accesses will not be copied.
    //
    llvm::Function* clone(llvm::Function* fn);

    // Wrap `fn` with helper_slice_fn function.
    //
    llvm::Function* make_slice(llvm::Function* fn);

    // Wrap `fn` with helper_lifted_fn function.
    //
    llvm::Function* make_final(llvm::Function* fn, uint64_t vip);

    // Current basic block function that is being lifted.
    //
    llvm::Function* function;

    //
    //
    llvm::IRBuilder<> ir;

    // Resolved semantics from intrinsics module and their functions.
    //
    std::unordered_map<std::string, llvm::Function*> sems;

    // Helpers loaded from intrinsics file.
    //
    llvm::Function* helper_lifted_fn;
    llvm::Function* helper_empty_block_fn;
    llvm::Function* helper_block_fn;
    llvm::Function* helper_slice_fn;
    llvm::Function* helper_keep_fn;
    llvm::Value*    helper_undef;

    llvm::LLVMContext context;
    std::unique_ptr<llvm::Module> module;
};

```

`src/logger.hpp`:

```hpp
#pragma once
#include <fmt/core.h>
#include <fmt/color.h>
#include <fmt/chrono.h>
#include <triton/ast.hpp>
#include <triton/context.hpp>

#include <type_traits>
#include <source_location>

#ifdef _MSC_VER
    #include <intrin.h>
    #define unreachable() __assume(0)
#else
    #include <emmintrin.h>
    #include <x86intrin.h>
    #define unreachable() __builtin_unreachable()
#endif

template <>
struct fmt::formatter<triton::ast::SharedAbstractNode> : fmt::formatter<std::string>
{
    template<typename fmtcontext>
    auto format(const triton::ast::SharedAbstractNode& node, fmtcontext& ctx) const
    {
        std::stringstream ss;
        ss << node;
        return formatter<std::string>::format(ss.str(), ctx);
    }
};

template<>
struct fmt::formatter<triton::engines::symbolic::SharedSymbolicVariable> : fmt::formatter<std::string>
{
    template<typename fmtcontext>
    auto format(const triton::engines::symbolic::SharedSymbolicVariable& var, fmtcontext& ctx) const
    {
        auto alias = var->getAlias();
        return formatter<std::string>::format(alias.empty() ? var->getName() : alias, ctx);
    }
};

template<>
struct fmt::formatter<triton::engines::symbolic::SharedSymbolicExpression> : fmt::formatter<std::string>
{
    template<typename fmtcontext>
    auto format(const triton::engines::symbolic::SharedSymbolicExpression& expr, fmtcontext& ctx) const
    {
        return formatter<std::string>::format(expr->getFormattedExpression(), ctx);
    }
};

template<>
struct fmt::formatter<triton::arch::Instruction> : fmt::formatter<std::string>
{
    template<typename fmtcontext>
    auto format(const triton::arch::Instruction& ins, fmtcontext& ctx) const
    {
        return formatter<std::string>::format(fmt::format("0x{:x} {}", ins.getAddress(), ins.getDisassembly()), ctx);
    }
};

template<>
struct fmt::formatter<triton::arch::Register> : fmt::formatter<std::string>
{
    template<typename fmtcontext>
    auto format(const triton::arch::Register& reg, fmtcontext& ctx) const
    {
        return formatter<std::string>::format(reg.getName(), ctx);
    }
};

namespace logger
{
static void debug(const char* format, auto... args)
{
    fmt::print(fg(fmt::color::dark_orange) | fmt::emphasis::bold, "[DEBUG] ");
    fmt::print(fmt::runtime(format), args...);
    fmt::print("\n");
}

static void info(const char* format, const auto&... args)
{
    fmt::print(fg(fmt::color::cadet_blue) | fmt::emphasis::bold, "[INFO]  ");
    fmt::print(fmt::runtime(format), args...);
    fmt::print("\n");
}

static void warn(const char* format, const auto&... args)
{
    fmt::print(fg(fmt::color::yellow) | fmt::emphasis::bold, "[WARN]  ");
    fmt::print(fmt::runtime(format), args...);
    fmt::print("\n");
}

static void error [[noreturn]](const char* format, const auto&... args)
{
    fmt::print(fg( fmt::color::red) | fmt::emphasis::bold, "[ERROR] ");
    fmt::print(fmt::runtime(format), args...);
    fmt::print("\n");
    // Never return.
    //
    unreachable();
}
};

```

`src/main.cpp`:

```cpp
#include "il/optimizer.hpp"
#include "explorer.hpp"
#include "emulator.hpp"
#include "logger.hpp"
#include "binary.hpp"
#include "utils.hpp"

#include <llvm/Support/Signals.h>
#include <llvm/Support/CommandLine.h>
#include <llvm/Support/PrettyStackTrace.h>

#include <fstream>

llvm::cl::opt<uint64_t> entrypoint("e",
    llvm::cl::desc("Virtual address of vmenter"),
    llvm::cl::value_desc("entrypoint"),
    llvm::cl::Required);

llvm::cl::opt<std::string> output("o",
    llvm::cl::desc("Path to the output .ll file"),
    llvm::cl::value_desc("output"),
    llvm::cl::init("output.ll"),
    llvm::cl::Optional);

// Necessary command line optimization flags for llvm. Thank you Matteo.
//
static const std::vector<const char*> optimization_args =
{
    "-memdep-block-scan-limit=100000",
    "-rotation-max-header-size=100000",
    "-earlycse-mssa-optimization-cap=1000000",
    "-dse-memoryssa-defs-per-block-limit=1000000",
    "-dse-memoryssa-partial-store-limit=1000000",
    "-dse-memoryssa-path-check-limit=1000000",
    "-dse-memoryssa-scanlimit=1000000",
    "-dse-memoryssa-walklimit=1000000",
    "-dse-memoryssa-otherbb-cost=2",
    "-memssa-check-limit=1000000",
    "-memdep-block-number-limit=1000000",
    "-memdep-block-scan-limit=1000000",
    "-gvn-max-block-speculations=1000000",
    "-gvn-max-num-deps=1000000",
    "-gvn-hoist-max-chain-length=-1",
    "-gvn-hoist-max-depth=-1",
    "-gvn-hoist-max-bbs=-1",
    "-unroll-threshold=1000000"
};

int main(int argc, char** argv)
{
    // Inject optimization options to argv.
    //
    std::vector<const char*> args;
    std::copy_n(argv, argc, std::back_inserter(args));
    std::copy(optimization_args.begin(), optimization_args.end(), std::back_inserter(args));
    // Enable stack traces.
    //
    llvm::sys::PrintStackTraceOnErrorSignal(args[0]);
    llvm::PrettyStackTraceProgram X(args.size(), args.data());
    // Parse command parameters.
    //
    llvm::cl::ParseCommandLineOptions(args.size(), args.data());

    auto lifter = std::make_shared<Lifter>();
    auto tracer = std::make_shared<Tracer>(triton::arch::architecture_e::ARCH_X86_64);
    Explorer explorer(lifter, tracer);

    auto rtn = explorer.explore(entrypoint);
    auto fn  = lifter->build_function(rtn.get());

    il::optimize_virtual_function(fn);

    save_ir(fn, fmt::format("function.{}", output));

    return 0;
}

```

`src/tracer.cpp`:

```cpp
#include "asserts.hpp"
#include "tracer.hpp"
#include "logger.hpp"
#include "utils.hpp"

namespace variable
{
    static const std::string rsp          = "rsp";
    static const std::string vip          = "vip";
    static const std::string vip_fetch    = "[vip]";
    static const std::string vsp          = "vsp";
    static const std::string vsp_fetch    = "[vsp]";
    static const std::string vregs        = "vregs";
    static const std::string memory_fetch = "[memory]";
};

// Match [vsp] + [vsp].
//
static bool match_add(const triton::ast::SharedAbstractNode& ast)
{
    if (ast->getType() == triton::ast::EXTRACT_NODE)
    {
        return match_add(ast->getChildren()[2]->getChildren()[1]);
    }
    return ast->getType() == triton::ast::BVADD_NODE
        && is_variable(ast->getChildren()[1], variable::vsp_fetch);
}

// Match `~[vsp] | ~[vsp]`.
//
static bool match_nand(const triton::ast::SharedAbstractNode& ast)
{
    // For nand_8 ast is following:
    // ((_ extract 15 0) (concat ((_ extract 63 8) (concat (_ bv0 48) [vsp])) (bvor (bvnot ((_ extract 7 0) [vsp])) (bvnot [vsp]))))
    //
    if (ast->getType() == triton::ast::EXTRACT_NODE)
    {
        return match_nand(ast->getChildren()[2]->getChildren()[1]);
    }
    return ast->getType() == triton::ast::BVOR_NODE
        && ast->getChildren()[1]->getType() == triton::ast::BVNOT_NODE
        && is_variable(ast->getChildren()[1]->getChildren()[0], variable::vsp_fetch);
}

// Match `~[vsp] & ~[vsp]`.
//
static bool match_nor(const triton::ast::SharedAbstractNode& ast)
{
    // For nor_8 ast is following:
    // ((_ extract 15 0) (concat ((_ extract 63 8) (concat (_ bv0 48) [vsp])) (bvand (bvnot ((_ extract 7 0) [vsp])) (bvnot [vsp]))))
    //
    if (ast->getType() == triton::ast::EXTRACT_NODE)
    {
        return match_nor(ast->getChildren()[2]->getChildren()[1]);
    }
    return ast->getType() == triton::ast::BVAND_NODE
        && ast->getChildren()[1]->getType() == triton::ast::BVNOT_NODE
        && is_variable(ast->getChildren()[1]->getChildren()[0], variable::vsp_fetch);
}

// Match `[vsp] >> ([vsp] & 0x3f)`.
//
static bool match_shr(const triton::ast::SharedAbstractNode& ast)
{
    // for shr
    if (ast->getType() == triton::ast::EXTRACT_NODE && ast->getChildren()[2]->getType() == triton::ast::CONCAT_NODE)
    {
        return ast->getChildren()[2]->getChildren()[1]->getType() == triton::ast::BVLSHR_NODE;
    }
    return ast->getType() == triton::ast::BVLSHR_NODE
        && ast->getChildren()[1]->getType() == triton::ast::BVAND_NODE
        && is_variable(ast->getChildren()[0], variable::vsp_fetch);
}

// Match `[vsp] << ([vsp] & 0x3f)`.
//
static bool match_shl(const triton::ast::SharedAbstractNode& ast)
{
    // for shl_8: ((_ extract 15 0) (concat ((_ extract 63 8) (concat (_ bv281474976710649 48) [vsp])) (bvshl ((_ extract 7 0) [vsp]) (bvand [vsp] (_ bv31 8)))))
    //
    if (ast->getType() == triton::ast::EXTRACT_NODE && ast->getChildren()[2]->getType() == triton::ast::CONCAT_NODE)
    {
        return ast->getChildren()[2]->getChildren()[1]->getType() == triton::ast::BVSHL_NODE;
    }
    return ast->getType() == triton::ast::BVSHL_NODE
        && ast->getChildren()[1]->getType() == triton::ast::BVAND_NODE
        && is_variable(ast->getChildren()[0], variable::vsp_fetch);
}

// Match `ror((([vsp]) << 32 | [vsp]), 0x0, 64)`
static bool match_shrd(const triton::ast::SharedAbstractNode& ast)
{
    return ast->getType() == triton::ast::EXTRACT_NODE
        && ast->getChildren()[2]->getType() == triton::ast::BVROR_NODE;
}

// Match `((_ extract 31 0) ((_ rotate_left 0) (concat [vsp] [vsp])))`.
//
static bool match_shld(const triton::ast::SharedAbstractNode& ast)
{
    return ast->getType() == triton::ast::EXTRACT_NODE
        && ast->getChildren()[2]->getType() == triton::ast::BVROL_NODE;
}

Tracer::Tracer(triton::arch::architecture_e arch) noexcept
    : Emulator(arch)
{
    physical_registers_count = (arch == triton::arch::ARCH_X86_64 ? 16 : 8);
}

Tracer::Tracer(Tracer const& other) noexcept
    : Emulator(other)
    , physical_registers_count{ other.physical_registers_count }
    , vip_register_name       { other.vip_register_name        }
    , vsp_register_name       { other.vsp_register_name        }
{
}

uint64_t Tracer::vip() const
{
    return read(vip_register());
}

uint64_t Tracer::vsp() const
{
    return read(vsp_register());
}

const triton::arch::Register& Tracer::vip_register() const
{
    fassert(vip_register_name.has_value());
    return getRegister(vip_register_name.value());
}

const triton::arch::Register& Tracer::vsp_register() const
{
    fassert(vsp_register_name.has_value());
    return getRegister(vsp_register_name.value());
}

std::shared_ptr<Tracer> Tracer::fork() const noexcept
{
    return std::make_shared<Tracer>(*this);
}

vm::Instruction Tracer::step(step_t type)
{
    auto tracer = fork();

    if (auto vinsn_mb = tracer->process_instruction())
    {
        auto vinsn = vinsn_mb.value();
        if (vm::op_enter(vinsn))
        {
            vip_register_name = tracer->vip_register_name;
            vsp_register_name = tracer->vsp_register_name;
        }
        if (vm::op_branch(vinsn) && type == step_t::stop_before_branch)
            return vinsn;

        if (vm::op_jcc(vinsn))
        {
            vip_register_name = std::get<vm::Jcc>(vinsn).vip_register();
            vsp_register_name = std::get<vm::Jcc>(vinsn).vsp_register();
        }
        // Cicle tracer to the current rip value.
        //
        while (true)
        {
            auto insn = disassemble();
            if (insn.getAddress() == tracer->rip())
                break;
            execute(insn);
        }
        return vinsn;
    }
    logger::error("Failed to process instruction");
}

std::optional<vm::Instruction> Tracer::process_instruction()
{
    if (!vip_register_name.has_value() || !vsp_register_name.has_value())
    {
        return process_vmenter();
    }
    // List of matched virtual instructions for this handler.
    //
    std::vector<vm::Instruction> vinsn;
    // List of executed instructions.
    //
    std::vector<triton::arch::Instruction> stream;
    // Symbolize bytecode and virtual stack.
    //
    symbolizeRegister(vip_register(), "vip");
    symbolizeRegister(vsp_register(), "vsp");
    symbolizeRegister(rsp_register(), "rsp");

    cache.clear();
    std::set<std::string> poped_registers;
    std::vector<vm::Pop>  poped_context;

    while (true)
    {
        auto insn = disassemble();
        // Handle memory write.
        //
        if (op_mov_memory_register(insn))
        {
            getSymbolicEngine()->initLeaAst(insn.operands[0].getMemory());
            if (auto vins = process_store(insn))
            {
                vinsn.push_back(std::move(vins.value()));
            }
        }
        // Handle pop register.
        //
        else if (op_pop_register(insn))
        {
            const auto& reg = insn.operands[0].getConstRegister();
            const auto name = reg.getName();
            if (!poped_registers.contains(name))
            {
                poped_context.push_back(vm::Pop(vm::PhysicalRegister(name), reg.getBitSize()));
                poped_registers.insert(std::move(name));
            }
        }
        else if (op_pop_flags(insn))
        {
            if (!poped_registers.contains("eflags"))
            {
                poped_context.push_back(vm::Pop(vm::PhysicalRegister("eflags"), 8 * ptrsize()));
                poped_registers.insert("eflags");
            }
        }
        // Build instruction semantics.
        //
        execute(insn);
        // Handle memory read.
        //
        if (op_mov_register_memory(insn))
        {
            if (auto vins = process_load(insn))
            {
                vinsn.push_back(std::move(vins.value()));
            }
        }

        if (op_ret(insn) && poped_registers.size() == physical_registers_count)
        {
            stream.push_back(std::move(insn));
            break;
        }
        stream.push_back(std::move(insn));

        auto variables = collect_variables(getRegisterAst(rip_register()));

        if (has_variable(variables, variable::vip_fetch) ||
            has_variable(variables, variable::memory_fetch, variable::vsp_fetch))
        {
            break;
        }
        if (!rip())
        {
            break;
        }
    }

    if (vinsn.empty())
    {
        const auto variables = collect_variables(getRegisterAst(rip_register()));
        if (has_variable(variables, variable::memory_fetch, variable::vsp_fetch))
        {
            // Jcc handler.
            //
            auto comment   = get_variable(variables, variable::memory_fetch).value()->getComment();
            auto vip_reg   = getRegister(comment);
            auto vip_ast   = triton::ast::unroll(getRegisterAst(vip_reg));
            auto direction = vip_ast->getType() == triton::ast::BVADD_NODE ? vm::jcc_e::up : vm::jcc_e::down;
            // Pick next handler and deduce vsp register. We know that the first instruction after jcc is pop so
            // first memory access should be access to vsp.
            auto tracer = fork();
            for (int i = 0; i < 10; i++)
            {
                auto insn = tracer->single_step();
                if (op_mov_register_memory(insn))
                {
                    auto& vsp_reg = insn.operands[1].getConstMemory().getConstBaseRegister();
                    return vm::Jcc(
                        direction,
                        vip_reg.getName(),
                        vsp_reg.getName()
                    );
                }
            }
            fassert("Failed to process jcc instruction.");
        }
        else if (has_variable(variables, variable::vip_fetch) && ranges::any_of(stream, op_lea_rip))
        {
            // Jmp handler.
            //
            vinsn.push_back(vm::Jmp());
        }
        else if (poped_context.size() == physical_registers_count)
        {
            // Exit handler.
            //
            return vm::Exit(std::move(poped_context));
        }
    }
    if (vinsn.size() != 1)
    {
        for (const auto& insn : stream)
            logger::warn("{}", insn);
        return {};
    }
    return vinsn.at(0);
}

std::optional<vm::Instruction> Tracer::process_vmenter()
{
    // Save rsp for future lookup.
    //
    const auto rsp_value = rsp();
    // Symbolize initial context.
    //
    for (const auto& reg : regs())
    {
        symbolizeRegister(reg, reg.getName());
    }
    std::vector<triton::arch::Instruction> stream;
    // Execute vmenter. Collect virtual registers.
    //
    while (true)
    {
        auto insn = single_step();

        if (op_mov_register_register(insn))
        {
            const auto& r1 = insn.operands[0].getConstRegister();
            const auto& r2 = insn.operands[1].getConstRegister();

            if (r2 == rsp_register() && r1.getBitSize() == r2.getBitSize())
            {
                vsp_register_name = r1.getName();
            }
        }
        else if (op_mov_register_memory(insn))
        {
            const auto& r1 = insn.operands[0].getConstRegister();
            const auto& r2 = insn.operands[1].getConstMemory().getConstBaseRegister();
            if (r2 != rsp_register())
            {
                vip_register_name = r2.getName();
            }
            symbolizeRegister(r1, r1.getName());
        }
        stream.push_back(std::move(insn));
        if (isRegisterSymbolized(rip_register()))
            break;
    }

    if (!vip_register_name.has_value() || !vsp_register_name.has_value())
    {
        logger::warn("No virtual registers were found:");
        logger::warn("\tvip: {}", vip_register_name.has_value() ? "found" : "not found");
        logger::warn("\tvsp: {}", vsp_register_name.has_value() ? "found" : "not found");

        for (const auto& insn : stream)
            logger::warn("{}", insn);
        return {};
    }

    // Number of pushed physical registers on vmenter + 2 integers before vmenter and reloc at the end.
    //
    const auto context_size = physical_registers_count + 3;
    // Collect initial context.
    //
    std::vector<vm::Push> context;
    for (uint64_t addr = rsp_value - ptrsize(); addr >= rsp_value - context_size * ptrsize(); addr -= ptrsize())
    {
        triton::arch::MemoryAccess memory(addr, ptrsize());
        if (isMemorySymbolized(memory))
        {
            auto ast  = triton::ast::unroll(getMemoryAst(memory));
            auto size = ast->getBitvectorSize();
            fassert(ast->getType() == triton::ast::VARIABLE_NODE);
            context.push_back(vm::Push(vm::PhysicalRegister(to_variable(ast)->getAlias()), size));
        }
        else
        {
            // Match eflags since its not symbolic.
            //
            if (auto off = rsp_value - addr; off > 2 * ptrsize() && off < context_size * ptrsize())
            {
                context.push_back(vm::Push(vm::PhysicalRegister("eflags"), 8 * ptrsize()));
            }
            else
            {
                context.push_back(vm::Push(vm::Immediate(read<uint64_t>(memory)), 8 * ptrsize()));
            }
        }
    }
    if (context.size() != context_size)
        return {};
    return vm::Enter(std::move(context));
}

std::optional<vm::Instruction> Tracer::process_store(const triton::arch::Instruction& insn)
{
    const auto& mem    = insn.operands[0].getConstMemory();
    const auto& reg    = insn.operands[1].getConstRegister();
    auto mem_ast       = triton::ast::unroll(mem.getLeaAst());
    auto reg_ast       = triton::ast::unroll(getRegisterAst(reg));
    auto mem_variables = collect_variables(mem_ast);
    auto reg_variables = collect_variables(reg_ast);

    auto size = reg_ast->getBitvectorSize();

    if (reg_ast->getType() == triton::ast::EXTRACT_NODE && size == 16 && !has_variable(reg_variables, variable::vsp))
    {
        size = 8;
    }

    // movzx ax, byte ptr [vsp]
    // mov [vmregs + offset], ax
    //
    if (has_variable(mem_variables, variable::rsp, variable::vip_fetch) &&
        has_variable(reg_variables, variable::vsp_fetch))
    {
        auto write_off = read(mem.getConstIndexRegister());
        auto number    = write_off / ptrsize();
        auto offset    = write_off % ptrsize();
        auto original  = lookup_instruction(get_variable(reg_variables, variable::vsp_fetch).value());
        return vm::Pop(vm::VirtualRegister(number, offset), original.operands[1].getBitSize());
    }
    if (has_variable(mem_variables, variable::vsp) &&
        has_variable(reg_variables, variable::vip_fetch))
    {
        return vm::Push(vm::Immediate(static_cast<uint64_t>(reg_ast->evaluate())), reg.getBitSize());
    }
    // mov ax, byte ptr [vmregs + offset]
    // mov [vsp], ax
    //
    if (has_variable(mem_variables, variable::vsp) &&
        has_variable(reg_variables, variable::vregs))
    {
        auto vreg = get_variable(reg_variables, variable::vregs).value();
        uint64_t index{};
        if (std::sscanf(vreg->getComment().c_str(), "0x%lx", &index) != 1)
        {
            logger::error("Failed to parse comment of push vreg instruction: {}", vreg->getComment());
        }
        auto number   = index / ptrsize();
        auto offset   = index % ptrsize();
        auto original = lookup_instruction(get_variable(reg_variables, variable::vregs).value());
        return vm::Push(vm::VirtualRegister(number, offset), original.operands[1].getBitSize());
    }
    if (has_variable(mem_variables, variable::vsp) &&
        has_variable(reg_variables, variable::vsp))
    {
        return vm::Push(vm::VirtualStackPointer(), mem.getBitSize());
    }
    if (has_variable(mem_variables, variable::vsp_fetch) &&
        has_variable(reg_variables, variable::vsp_fetch))
    {
        return vm::Str(mem.getBitSize());
    }
    if (has_variable(mem_variables, variable::vsp) &&
        has_variable(reg_variables, variable::memory_fetch))
    {
        auto original = lookup_instruction(get_variable(reg_variables, variable::memory_fetch).value());
        return vm::Ldr(original.operands[1].getBitSize());
    }
    if (has_variable(mem_variables, variable::vsp) && match_add(reg_ast))
    {
        return vm::Add(size);
    }
    if (has_variable(mem_variables, variable::vsp) && match_nand(reg_ast))
    {
        return vm::Nand(size);
    }
    if (has_variable(mem_variables, variable::vsp) && match_nor(reg_ast))
    {
        return vm::Nor(size);
    }
    if (has_variable(mem_variables, variable::vsp) && match_shr(reg_ast))
    {
        return vm::Shr(size);
    }
    if (has_variable(mem_variables, variable::vsp) && match_shl(reg_ast))
    {
        return vm::Shl(size);
    }
    if (has_variable(mem_variables, variable::vsp) && match_shrd(reg_ast))
    {
        return vm::Shrd(size);
    }
    if (has_variable(mem_variables, variable::vsp) && match_shld(reg_ast))
    {
        return vm::Shld(size);
    }
    logger::warn("Failed to match store at 0x{:x}:", rip());
    logger::warn("\tmemory   AST: {}", mem_ast);
    logger::warn("\tregister AST: {}", reg_ast);
    return {};
}

std::optional<vm::Instruction> Tracer::process_load(const triton::arch::Instruction& insn)
{
    const auto& reg = insn.operands[0].getConstRegister();
    const auto& mem = insn.operands[1].getConstMemory();
    const auto variables = collect_variables(mem.getLeaAst());

    if (has_variable(variables, variable::vip))
    {
        cache_instruction(insn, symbolizeRegister(reg, variable::vip_fetch));
    }
    else if (has_variable(variables, variable::vsp))
    {
        cache_instruction(insn, symbolizeRegister(reg, variable::vsp_fetch));

        if (vsp_register().isOverlapWith(reg))
        {
            return vm::Pop(vm::VirtualStackPointer(), mem.getBitSize());
        }
    }
    else if (has_variable(variables, variable::rsp, variable::vip_fetch))
    {
        // Set read offset as a comment to symbolic variable. It is used as vreg index in push vreg handler.
        //
        auto var = symbolizeRegister(reg, variable::vregs);
        var->setComment(fmt::format("0x{:x}", read(mem.getConstIndexRegister())));
        cache_instruction(insn, var);
    }
    else if (has_variable(variables, variable::vsp_fetch))
    {
        // Set memory operand register name as a comment to symbolic variable. It is used as new vip register in jcc handler.
        //
        auto var = symbolizeRegister(reg, variable::memory_fetch);
        var->setComment(fmt::format("{}", mem.getConstBaseRegister().getName()));
        cache_instruction(insn, var);
    }
    return {};
}

void Tracer::cache_instruction(triton::arch::Instruction insn, triton::engines::symbolic::SharedSymbolicVariable variable)
{
    cache.emplace(variable, insn);
}

const triton::arch::Instruction& Tracer::lookup_instruction(triton::engines::symbolic::SharedSymbolicVariable variable) const
{
    if (cache.find(variable) != cache.end())
        return cache.at(variable);
    logger::error("no instruction was found for {} variable", variable);
}

```

`src/tracer.hpp`:

```hpp
#pragma once

#include "emulator.hpp"
#include "vm/instruction.hpp"

enum class step_t
{
    stop_before_branch,
    execute_branch
};

struct Tracer final : Emulator
{
    Tracer(triton::arch::architecture_e arch) noexcept;
    Tracer(Tracer const& other) noexcept;

    std::shared_ptr<Tracer> fork() const noexcept;

    uint64_t vip() const;
    uint64_t vsp() const;

    const triton::arch::Register& vip_register() const;
    const triton::arch::Register& vsp_register() const;

    vm::Instruction step(step_t type);

private:
    std::optional<vm::Instruction> process_instruction();
    std::optional<vm::Instruction> process_vmenter();
    std::optional<vm::Instruction> process_store(const triton::arch::Instruction& insn);
    std::optional<vm::Instruction> process_load (const triton::arch::Instruction& insn);

    void cache_instruction(triton::arch::Instruction insn, triton::engines::symbolic::SharedSymbolicVariable variable);
    const triton::arch::Instruction& lookup_instruction(triton::engines::symbolic::SharedSymbolicVariable variable) const;

    // Context size based on architecture.
    //
    size_t physical_registers_count;

    std::optional<std::string> vip_register_name;
    std::optional<std::string> vsp_register_name;

    std::unordered_map<triton::engines::symbolic::SharedSymbolicVariable, triton::arch::Instruction> cache;
};

```

`src/utils.cpp`:

```cpp
#include "utils.hpp"

#include <llvm/IR/Module.h>

std::vector<triton::engines::symbolic::SharedSymbolicVariable> collect_variables(const triton::ast::SharedAbstractNode& ast)
{
    using namespace triton::ast;
    const auto vec = childrenExtraction(ast, true, true);
    return vec
        | ranges::views::filter   ([](const SharedAbstractNode& node){ return node->getType() == VARIABLE_NODE; })
        | ranges::views::transform([](const SharedAbstractNode& node){ return std::dynamic_pointer_cast<VariableNode>(node)->getSymbolicVariable(); })
        | ranges::to_vector;
}

bool is_variable(const triton::ast::SharedAbstractNode& node, const std::string& alias)
{
    if (alias.empty())
        return node->getType() == triton::ast::VARIABLE_NODE;
    return node->getType() == triton::ast::VARIABLE_NODE
        && std::dynamic_pointer_cast<triton::ast::VariableNode>(node)->getSymbolicVariable()->getAlias() == alias;
}

triton::engines::symbolic::SharedSymbolicVariable to_variable(const triton::ast::SharedAbstractNode& node)
{
    return std::dynamic_pointer_cast<triton::ast::VariableNode>(node)->getSymbolicVariable();
}

bool op_mov_register_register(const triton::arch::Instruction& insn)
{
    return (insn.getType() == triton::arch::x86::ID_INS_MOV
        ||  insn.getType() == triton::arch::x86::ID_INS_MOVZX
        ||  insn.getType() == triton::arch::x86::ID_INS_MOVSX)
        &&  insn.operands[0].getType() == triton::arch::OP_REG
        &&  insn.operands[1].getType() == triton::arch::OP_REG;
}

bool op_mov_register_memory(const triton::arch::Instruction& insn)
{
    return (insn.getType() == triton::arch::x86::ID_INS_MOV
        ||  insn.getType() == triton::arch::x86::ID_INS_MOVZX
        ||  insn.getType() == triton::arch::x86::ID_INS_MOVSX)
        &&  insn.operands[0].getType() == triton::arch::OP_REG
        &&  insn.operands[1].getType() == triton::arch::OP_MEM;
}

bool op_mov_memory_register(const triton::arch::Instruction& insn)
{
    return (insn.getType() == triton::arch::x86::ID_INS_MOV
        ||  insn.getType() == triton::arch::x86::ID_INS_MOVZX
        ||  insn.getType() == triton::arch::x86::ID_INS_MOVSX)
        &&  insn.operands[0].getType() == triton::arch::OP_MEM
        &&  insn.operands[1].getType() == triton::arch::OP_REG;
}

bool op_pop_register(const triton::arch::Instruction& insn)
{
    return insn.getType() == triton::arch::x86::ID_INS_POP
        && insn.operands[0].getType() == triton::arch::OP_REG;
}

bool op_jmp_register(const triton::arch::Instruction& insn)
{
    return insn.getType() == triton::arch::x86::ID_INS_JMP
        && insn.operands[0].getType() == triton::arch::OP_REG;
}

bool op_pop_flags(const triton::arch::Instruction& insn)
{
    return insn.getType() == triton::arch::x86::ID_INS_POPFQ || insn.getType() == triton::arch::x86::ID_INS_POPFD;
}

bool op_lea_rip(const triton::arch::Instruction& insn)
{
    return insn.getType() == triton::arch::x86::ID_INS_LEA
        && insn.operands[1].getConstMemory().getConstBaseRegister().getId() == triton::arch::ID_REG_X86_RIP
        && insn.operands[1].getConstMemory().getConstDisplacement().getValue() == -7;
}

bool op_ret(const triton::arch::Instruction& insn)
{
    return insn.getType() == triton::arch::x86::ID_INS_RET;
}

void save_ir(llvm::Value* value, const std::string& filename)
{
    std::error_code ec;
    llvm::raw_fd_ostream fd(filename, ec);
    value->print(fd, false);
}

void save_ir(llvm::Module* module, const std::string& filename)
{
    std::error_code ec;
    llvm::raw_fd_ostream fd(filename, ec);
    module->print(fd, nullptr);
}

```

`src/utils.hpp`:

```hpp
#pragma once

#include <triton/ast.hpp>
#include <triton/instruction.hpp>
#include <triton/symbolicEngine.hpp>
#include <triton/x86Specifications.hpp>

#include <range/v3/view/filter.hpp>
#include <range/v3/view/transform.hpp>
#include <range/v3/range/conversion.hpp>
#include <range/v3/algorithm/any_of.hpp>
#include <range/v3/algorithm/find_if.hpp>

#include <llvm/IR/Function.h>
#include <optional>

bool has_variable(const auto& range, const std::string& alias)
{
    return ranges::any_of(range, [&](const auto& node) { return node->getAlias() == alias; });
}

bool has_variable(const auto& range, const std::string& alias, const auto&... args)
{
    return range.size() == 1 + sizeof...(args) && has_variable(range, alias) && has_variable(range, args...);
}

auto get_variable(const auto& range, const std::string& alias) -> std::optional<triton::engines::symbolic::SharedSymbolicVariable>
{
    auto var = ranges::find_if(range, [&](const auto& var){ return var->getAlias() == alias; });
    if (var != ranges::end(range))
        return *var;
    return {};
}

auto collect_variables(const triton::ast::SharedAbstractNode& ast) -> std::vector<triton::engines::symbolic::SharedSymbolicVariable>;
bool is_variable(const triton::ast::SharedAbstractNode& node, const std::string& alias = "");
auto to_variable(const triton::ast::SharedAbstractNode& node) -> triton::engines::symbolic::SharedSymbolicVariable;

// Matches mov/movzx/movsx register, register.
//
bool op_mov_register_register(const triton::arch::Instruction& insn);

// Matches mov/movzx/movsx [memory], register.
//
bool op_mov_memory_register(const triton::arch::Instruction& insn);

// Matches mov/movzx/movsx register, [memory].
//
bool op_mov_register_memory(const triton::arch::Instruction& insn);

// Matches pop register.
//
bool op_pop_register(const triton::arch::Instruction& insn);

// Matches jmp register.
//
bool op_jmp_register(const triton::arch::Instruction& insn);

// Matches popfq/popfd.
//
bool op_pop_flags(const triton::arch::Instruction& insn);

// Matches `lea register, [rip - 7]`
//
bool op_lea_rip(const triton::arch::Instruction& insn);

// Matches ret.
//
bool op_ret(const triton::arch::Instruction& insn);

void save_ir(llvm::Value* value,   const std::string& filename);
void save_ir(llvm::Module* module, const std::string& filename);

```

`src/vm/instruction.cpp`:

```cpp
#include "instruction.hpp"
#include "logger.hpp"

namespace vm
{
PhysicalRegister::PhysicalRegister(std::string name)
    : name_{ std::move(name) }
{
}

const std::string& PhysicalRegister::name() const noexcept
{
    return name_;
}

VirtualRegister::VirtualRegister(int number, int offset)
    : number_{ number }, offset_{ offset }
{
}

int VirtualRegister::number() const noexcept
{
    return number_;
}

int VirtualRegister::offset() const noexcept
{
    return offset_;
}

Immediate::Immediate(uint64_t value)
    : value_{ value }
{
}

uint64_t Immediate::value() const noexcept
{
    return value_;
}

Operand::Operand(Immediate&& imm)           noexcept : op{ std::move(imm) } {}
Operand::Operand(VirtualRegister&& reg)     noexcept : op{ std::move(reg) } {}
Operand::Operand(PhysicalRegister&& reg)    noexcept : op{ std::move(reg) } {}
Operand::Operand(VirtualStackPointer&& reg) noexcept : op{ std::move(reg) } {}

bool Operand::is_vsp() const noexcept
{
    return std::holds_alternative<VirtualStackPointer>(op);
}
bool Operand::is_virtual() const noexcept
{
    return std::holds_alternative<VirtualRegister>(op);
}
bool Operand::is_physical() const noexcept
{
    return std::holds_alternative<PhysicalRegister>(op);
}
bool Operand::is_immediate() const noexcept
{
    return std::holds_alternative<Immediate>(op);
}

const Immediate& Operand::imm() const noexcept
{
    return std::get<Immediate>(op);
}
const VirtualRegister& Operand::vrt() const noexcept
{
    return std::get<VirtualRegister>(op);
}
const PhysicalRegister& Operand::phy() const noexcept
{
    return std::get<PhysicalRegister>(op);
}
const VirtualStackPointer& Operand::vsp() const noexcept
{
    return std::get<VirtualStackPointer>(op);
}

std::string Operand::to_string() const noexcept
{
    if (is_immediate())
        return fmt::format("0x{:016x}", imm().value());
    else if (is_physical())
        return fmt::format("{}", phy().name());
    else if (is_virtual())
        return fmt::format("vmregs[{:02}:{:02}]", vrt().number(), vrt().offset());
    return fmt::format("{}", "vsp");
}

Sized::Sized(int size)
    : size_{ size }
{
}

int Sized::size() const noexcept
{
    return size_;
}

Push::Push(Operand&& operand, int size)
    : Sized(size), operand{ std::move(operand) }
{
}

const Operand& Push::op() const noexcept
{
    return operand;
}

Pop::Pop(Operand&& operand, int size)
    : Sized(size), operand{ std::move(operand) }
{
}

const Operand& Pop::op() const noexcept
{
    return operand;
}

Exit::Exit(std::vector<Pop> context)
    : context(std::move(context))
{
}

const std::vector<Pop>& Exit::regs() const noexcept
{
    return context;
}

Enter::Enter(std::vector<Push> context)
    : context(std::move(context))
{
}

const std::vector<Push>& Enter::regs() const noexcept
{
    return context;
}

Jcc::Jcc(jcc_e type, std::string vip, std::string vsp)
    : type{ type }, vip{ std::move(vip) }, vsp{ std::move(vsp) }
{
}

const std::string& Jcc::vip_register() const noexcept
{
    return vip;
}

const std::string& Jcc::vsp_register() const noexcept
{
    return vsp;
}

jcc_e Jcc::direction() const noexcept
{
    return type;
}

bool op_push_imm(const Instruction& insn)
{
    return std::holds_alternative<Push>(insn)
        && std::get<Push>(insn).op().is_immediate();
}

bool op_branch(const Instruction& insn)
{
    return std::holds_alternative<Jmp>(insn)
        || std::holds_alternative<Jcc>(insn)
        || std::holds_alternative<Exit>(insn);
}

bool op_enter(const Instruction& insn)
{
    return std::holds_alternative<Enter>(insn);
}

bool op_exit(const Instruction& insn)
{
    return std::holds_alternative<Exit>(insn);
}

bool op_pop(const Instruction& insn)
{
    return std::holds_alternative<Pop>(insn);
}

bool op_jmp(const Instruction& insn)
{
    return std::holds_alternative<Jmp>(insn);
}

bool op_jcc(const Instruction& insn)
{
    return std::holds_alternative<Jcc>(insn);
}
};

```

`src/vm/instruction.hpp`:

```hpp
#pragma once

#include <optional>
#include <variant>
#include <vector>
#include <string>
#include <cstdint>

struct Tracer;

namespace vm
{
enum class jcc_e
{
    up,
    down
};

struct PhysicalRegister
{
    explicit PhysicalRegister(std::string name);

    const std::string& name() const noexcept;

private:
    std::string name_;
};

struct VirtualRegister
{
    explicit VirtualRegister(int number, int offset);

    int number() const noexcept;
    int offset() const noexcept;

private:
    int number_;
    int offset_;
};

struct VirtualStackPointer
{
};

struct Immediate
{
    explicit Immediate(uint64_t value);

    uint64_t value() const noexcept;

private:
    uint64_t value_;
};

struct Operand
{
    Operand(Immediate&& imm) noexcept;
    Operand(VirtualRegister&& reg) noexcept;
    Operand(PhysicalRegister&& reg) noexcept;
    Operand(VirtualStackPointer&& reg) noexcept;

    bool is_vsp() const noexcept;
    bool is_virtual() const noexcept;
    bool is_physical() const noexcept;
    bool is_immediate() const noexcept;

    const Immediate& imm() const noexcept;
    const VirtualRegister& vrt() const noexcept;
    const PhysicalRegister& phy() const noexcept;
    const VirtualStackPointer& vsp() const noexcept;

    std::string to_string() const noexcept;

private:
    std::variant<PhysicalRegister, VirtualRegister, VirtualStackPointer, Immediate> op;
};

struct Sized
{
    Sized(int size);

    int size() const noexcept;

protected:
    int size_;
};

struct Add : public Sized
{
    using Sized::Sized;
};

struct Shl : public Sized
{
    using Sized::Sized;
};

struct Shr : public Sized
{
    using Sized::Sized;
};

struct Shrd : public Sized
{
    using Sized::Sized;
};

struct Shld : public Sized
{
    using Sized::Sized;
};

struct Ldr : public Sized
{
    using Sized::Sized;
};

struct Str : public Sized
{
    using Sized::Sized;
};

struct Nor : public Sized
{
    using Sized::Sized;
};

struct Nand : public Sized
{
    using Sized::Sized;
};

struct Push : public Sized
{
    Push(Operand&& operand, int size);

    const Operand& op() const noexcept;

private:
    Operand operand;
};

struct Pop : public Sized
{
    Pop(Operand&& operand, int size);

    const Operand& op() const noexcept;

private:
    Operand operand;
};

struct Jmp{};
struct Ret{};
struct Exit
{
    Exit(std::vector<Pop> context);

    const std::vector<Pop>& regs() const noexcept;

private:
    std::vector<Pop> context;
};

struct Enter
{
    Enter(std::vector<Push> context);

    const std::vector<Push>& regs() const noexcept;

private:
    std::vector<Push> context;
};

struct Jcc
{
    Jcc(jcc_e type, std::string vip, std::string vsp);

    const std::string& vip_register() const noexcept;
    const std::string& vsp_register() const noexcept;
    jcc_e              direction()    const noexcept;

private:
    jcc_e type;
    std::string vip;
    std::string vsp;
};

using Instruction = std::variant<Add, Nor, Nand, Shl, Shr, Shrd, Shld, Ldr, Str, Push, Pop, Jmp, Ret, Exit, Enter, Jcc>;

bool op_push_imm(const Instruction& insn);
bool op_branch(const Instruction& insn);
bool op_enter(const Instruction& insn);
bool op_exit(const Instruction& insn);
bool op_pop(const Instruction& insn);
bool op_jmp(const Instruction& insn);
bool op_jcc(const Instruction& insn);
};

```

`src/vm/routine.cpp`:

```cpp
#include "routine.hpp"
#include "asserts.hpp"
#include "logger.hpp"

namespace vm
{
BasicBlock::BasicBlock(uint64_t vip, Routine* rtn)
    : vip_(vip), owner(rtn), lifted(nullptr)
{
    owner->blocks.emplace(vip, this);
}

BasicBlock* BasicBlock::fork(uint64_t vip)
{
    if (owner->contains(vip))
    {
        next.push_back(owner->blocks.at(vip));
        return nullptr;
    }

    auto block = new BasicBlock(vip, owner);

    next.push_back(block);
    return block;
}

void BasicBlock::add(Instruction&& insn) noexcept
{
    vins.push_back(std::move(insn));
}

uint64_t BasicBlock::vip() const noexcept
{
    return vip_;
}

flow_t BasicBlock::flow() const noexcept
{
    if (vins.empty())
        return flow_t::unknown;
    else if (op_exit(vins.back()))
        return flow_t::exit;
    else if (op_jcc(vins.back()))
        return flow_t::conditional;
    else if (op_jmp(vins.back()))
        return flow_t::unconditional;
    return flow_t::unknown;
}

Routine::Routine(uint64_t vip)
{
    entry = new BasicBlock(vip, this);
}

Routine::~Routine()
{
    for (auto& [vip, block] : blocks)
        delete block;
}

BasicBlock* Routine::begin(uint64_t vip)
{
    return (new Routine(vip))->entry;
}

bool Routine::contains(uint64_t vip) const noexcept
{
    return blocks.count(vip) > 0;
}

std::string Routine::dot() const noexcept
{
    std::string body = "digraph g {\n";

    for (const auto& [vip, block] : blocks)
    {
        for (const auto& next : block->next)
            body += fmt::format("vip_0x{:08x} -> vip_0x{:08x} []\n", vip, next->vip());
    }
    return body + "}\n";
}
}

```

`src/vm/routine.hpp`:

```hpp
#pragma once

#include "instruction.hpp"

#include <set>
#include <string>
#include <vector>
#include <unordered_map>

namespace llvm
{
class Function;
}

namespace vm
{
static constexpr auto invalid_vip = ~0ull;

enum class flow_t
{
    exit,
    unknown,
    conditional,
    unconditional,
};

struct Routine;

struct BasicBlock
{
    BasicBlock(uint64_t vip, Routine* rtn);

    // Fork a new block from this block and link them together.
    //
    BasicBlock* fork(uint64_t vip);

    void add(Instruction&& insn) noexcept;

    auto begin() const noexcept { return vins.begin(); }
    auto end()   const noexcept { return vins.end();   }

    uint64_t vip() const noexcept;
    flow_t flow()  const noexcept;

    // Routine to which this block belongs to.
    //
    Routine* owner;

    llvm::Function* lifted;

    std::vector<BasicBlock*> next;

private:
    uint64_t vip_;

    std::vector<Instruction> vins;
};

struct Routine
{
    Routine(uint64_t vip);
    ~Routine();

    // Create a routine and return entry basic block.
    //
    static BasicBlock* begin(uint64_t vip);

    bool contains(uint64_t vip) const noexcept;

    // Build graphviz control-flow graph.
    //
    std::string dot() const noexcept;

    auto begin() const noexcept { return blocks.begin(); }
    auto end()   const noexcept { return blocks.end();   }

    // First block in this routine.
    //
    BasicBlock* entry;

    // Explored blocks.
    //
    std::unordered_map<uint64_t, BasicBlock*> blocks;
};
}

```